<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="do">
<meta property="og:type" content="website">
<meta property="og:title" content="yukai">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="yukai">
<meta property="og:description" content="do">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="wyukai">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>yukai</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">yukai</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/24/OD-DenseBox%E9%9A%8F%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/24/OD-DenseBox%E9%9A%8F%E8%AE%B0/" class="post-title-link" itemprop="url">OD-DenseBox随记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-24 17:56:57" itemprop="dateCreated datePublished" datetime="2022-01-24T17:56:57+08:00">2022-01-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-25 13:43:33" itemprop="dateModified" datetime="2022-01-25T13:43:33+08:00">2022-01-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="1-DenseBox"><a href="#1-DenseBox" class="headerlink" title="1. DenseBox"></a>1. DenseBox</h3><p>最近在看文本检测算法EAST，其核心思想与DenseBox很是相似，结合了Unet结构。DenseBox是比较早期的Anchor-Free目标检测算法，方法本身有很多值得借鉴的地方，很多思想也比较超前， 比端到端的训练和识别，多尺度特征融合，结合关键点增强检测效果等。</p>
<p><img src="../images/OD-DenseBox%E9%9A%8F%E8%AE%B0/image-20220124190818693.png" alt="image-20220124190818693"></p>
<p><img src="../images/OD-DenseBox%E9%9A%8F%E8%AE%B0/image-20220124191404862.png" alt="image-20220124191404862" style="zoom:80%;" /></p>
<p>整体架构如Figure1所示，测试时，输入图片大小为(mxnx3)，输出为（m/4 x n/4 *5），第一维s为分类置信度，后四维为像素位置至目标边界的距离。</p>
<h3 id="2-网络结构"><a href="#2-网络结构" class="headerlink" title="2. 网络结构"></a>2. 网络结构</h3><p><img src="../images/OD-DenseBox%E9%9A%8F%E8%AE%B0/image-20220124191824821.png" alt="image-20220124191824821" style="zoom:80%;" /></p>
<p>主干网络以VGG-19为基础，只采用前12层，后边层重新设计，con3_4上采样之后与con4_4进行多尺度特征融合，然后Head部分分为两个分支，其中一个为score_map（1-channel map for class score），第二个分支为boundingbox参数的回归分支（the relative position of bounding box by 4-channel map）。</p>
<h3 id="3-Loss设计"><a href="#3-Loss设计" class="headerlink" title="3. Loss设计"></a>3. Loss设计</h3><p>分类置信度和边界框回归均采用了L2损失，特别地，增加了平衡采样策略Balance Sampling。</p>
<ol>
<li><p>或略灰色（模糊）区域</p>
<p>所谓灰色区域，是指正负样本边界部分的像素点，因为在这些区域由于标注的样本是很难区分的，让其参与训练反而会降低模型的精度，因此这一部分不会参与训练，计算loss，在输出坐标空间中，对于每一个非正标记的像素，只要其半径范围2内存在任意一个带正标记的像素，则<script type="math/tex">{f_{ign}}</script>设置为1。</p>
</li>
<li><p>Hard Negative Mining</p>
<ul>
<li>计算整个patch的3600个所有样本点，并根据loss进行排序；</li>
<li>取其中的1%，也就是36个作为hard-negative样本；</li>
<li>随机采样36个负样本和hard-negative构成72个负样本；</li>
<li>随机采样72个正样本。</li>
</ul>
<p>使用上面策略得到的144个样本参与训练，将其掩码参数<script type="math/tex">{f_{sel}}</script>置为1，其余样本的置为0。</p>
</li>
</ol>
<p>最终，每个像素点位置的掩码设计为如下公式：</p>
<p><img src="../images/OD-DenseBox%E9%9A%8F%E8%AE%B0/image-20220124194415082.png" alt="image-20220124194415082"></p>
<p>总的Loss设计为：</p>
<p><img src="../images/OD-DenseBox%E9%9A%8F%E8%AE%B0/image-20220124194520881.png" alt="image-20220124194520881"></p>
<p>其中<script type="math/tex">\theta</script>为卷积网络的参数， <script type="math/tex">[y_i^* > 0]</script> 表示只有正样本参与bounding box的训练。</p>
<h3 id="4-标签设计"><a href="#4-标签设计" class="headerlink" title="4. 标签设计"></a>4. 标签设计</h3><p>这里只展示部分关键代码，详细的部分请参考源码哦。</p>
<ul>
<li>DenseBox中对采用后的正负样本Patch，分配标签， 1个score,  2个box的左上和右下坐标，4个关键点坐标</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># class DenseBoxDataset(data.Dataset) </span></span><br><span class="line"><span class="comment"># 正负样本标签设置</span></span><br><span class="line"><span class="comment"># judge postive or negative...</span></span><br><span class="line"><span class="keyword">if</span> data_1 == data_2 == data_3 == data_4 \</span><br><span class="line">        == data_5 == data_6 == data_7 == data_8 \</span><br><span class="line">        == data_9 == data_10 == data_11 == data_12 == <span class="number">0.0</span>:</span><br><span class="line">    </span><br><span class="line">    self.labels.append(torch.FloatTensor([<span class="number">0.0</span>]))</span><br><span class="line">    self.bboxes.append(torch.FloatTensor([<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]))</span><br><span class="line">    self.vertices.append(torch.FloatTensor([<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>,</span><br><span class="line">                                            <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 2 bbox corners</span></span><br><span class="line">    <span class="comment"># turn coordinate to 60×60 coordinate space, float</span></span><br><span class="line"></span><br><span class="line">    self.labels.append(torch.FloatTensor([<span class="number">1.0</span>]))</span><br><span class="line"></span><br><span class="line">    bbox_leftup_x = data_1 / <span class="number">4.0</span></span><br><span class="line">    bbox_leftup_y = data_2 / <span class="number">4.0</span></span><br><span class="line">    bbox_rightdown_x = data_3 / <span class="number">4.0</span></span><br><span class="line">    bbox_rightdown_y = data_4 / <span class="number">4.0</span></span><br><span class="line"></span><br><span class="line">    self.bboxes.append(torch.FloatTensor(np.array([</span><br><span class="line">        bbox_leftup_x,</span><br><span class="line">        bbox_leftup_y,</span><br><span class="line">        bbox_rightdown_x,</span><br><span class="line">        bbox_rightdown_y</span><br><span class="line">    ])))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4 vertices: 240×240 coordinate space, float</span></span><br><span class="line">    <span class="comment"># turn coordinate to 60×60 coordinate space, float</span></span><br><span class="line"></span><br><span class="line">    leftup_x = data_5 / <span class="number">4.0</span></span><br><span class="line">    leftup_y = data_6 / <span class="number">4.0</span></span><br><span class="line"></span><br><span class="line">    rightup_x = data_7 / <span class="number">4.0</span></span><br><span class="line">    rightup_y = data_8 / <span class="number">4.0</span></span><br><span class="line"></span><br><span class="line">    rightdown_x = data_9 / <span class="number">4.0</span></span><br><span class="line">    rightdown_y = data_10 / <span class="number">4.0</span></span><br><span class="line"></span><br><span class="line">    leftdown_x = data_11 / <span class="number">4.0</span></span><br><span class="line">    leftdown_y = data_12 / <span class="number">4.0</span></span><br><span class="line"></span><br><span class="line">    self.vertices.append(torch.FloatTensor(np.array([</span><br><span class="line">        leftup_x,</span><br><span class="line">        leftup_y,  <span class="comment"># leftup corner</span></span><br><span class="line">        rightup_x,</span><br><span class="line">        rightup_y,  <span class="comment"># rightup corner</span></span><br><span class="line">        rightdown_x,</span><br><span class="line">        rightdown_y,  <span class="comment"># rightdown corner</span></span><br><span class="line">        leftdown_x,</span><br><span class="line">        leftdown_y])))  <span class="comment"># leftdown corner</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>初始化score_map， 设置正值时，与论文稍有出入。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># init score map: N×1×60×60</span></span><br><span class="line"><span class="comment"># cls_map_gt = init_score(bboxes=bbox,labels=labels,ratio=0.3)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_score</span>(<span class="params">bboxes, labels, ratio=<span class="number">0.3</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    init for both positive patch and negative patch</span></span><br><span class="line"><span class="string">    :param bboxes:</span></span><br><span class="line"><span class="string">    :param labels:</span></span><br><span class="line"><span class="string">    :param ratio:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> bboxes.size()[<span class="number">0</span>] == labels.size(<span class="number">0</span>) <span class="keyword">and</span> \</span><br><span class="line">           bboxes.size() == torch.Size([bboxes.size(<span class="number">0</span>), <span class="number">4</span>]) <span class="keyword">and</span> \</span><br><span class="line">           labels.size() == torch.Size([labels.size(<span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    score_map = torch.zeros([bboxes.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">60</span>, <span class="number">60</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item_i, (coord, lb) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(bboxes.numpy(), labels.numpy())):</span><br><span class="line">        <span class="comment"># process each item in the batch</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> lb == <span class="number">0.0</span>:  <span class="comment"># negative patch sample</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        bbox_center_x = <span class="built_in">float</span>(coord[<span class="number">0</span>] + coord[<span class="number">2</span>]) * <span class="number">0.5</span></span><br><span class="line">        bbox_center_y = <span class="built_in">float</span>(coord[<span class="number">1</span>] + coord[<span class="number">3</span>]) * <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        bbox_w = coord[<span class="number">2</span>] - coord[<span class="number">0</span>]</span><br><span class="line">        bbox_h = coord[<span class="number">3</span>] - coord[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        org_x = <span class="built_in">int</span>(bbox_center_x - <span class="built_in">float</span>(ratio * bbox_w * <span class="number">0.5</span>) + <span class="number">0.5</span>)</span><br><span class="line">        org_y = <span class="built_in">int</span>(bbox_center_y - <span class="built_in">float</span>(ratio * bbox_h * <span class="number">0.5</span>) + <span class="number">0.5</span>)</span><br><span class="line">        end_x = <span class="built_in">int</span>(<span class="built_in">float</span>(org_x) + <span class="built_in">float</span>(ratio * bbox_w) + <span class="number">0.5</span>)</span><br><span class="line">        end_y = <span class="built_in">int</span>(<span class="built_in">float</span>(org_y) + <span class="built_in">float</span>(ratio * bbox_h) + <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            score_map[item_i, :, org_y: end_y + <span class="number">1</span>, org_x: end_x + <span class="number">1</span>] = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(e)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> score_map</span><br></pre></td></tr></table></figure>
<ul>
<li>初始化loc_map</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_loc</span>(<span class="params">bboxes, labels</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    init loc map including positive and negative samples</span></span><br><span class="line"><span class="string">                                        0         1          2            3</span></span><br><span class="line"><span class="string">    :param bboxes:  batch_size×4: leftup_x, leftup_y, rightdown_x, rightdown_y</span></span><br><span class="line"><span class="string">    :param bboxes:</span></span><br><span class="line"><span class="string">    :param labels:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> bboxes.size()[<span class="number">0</span>] == labels.size(<span class="number">0</span>) <span class="keyword">and</span> \</span><br><span class="line">           bboxes.size() == torch.Size([bboxes.size(<span class="number">0</span>), <span class="number">4</span>]) <span class="keyword">and</span> \</span><br><span class="line">           labels.size() == torch.Size([labels.size(<span class="number">0</span>), <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    loc_map = torch.zeros([bboxes.size(<span class="number">0</span>), <span class="number">4</span>, <span class="number">60</span>, <span class="number">60</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item_i, (coord, lb) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(bboxes.numpy(), labels.numpy())):</span><br><span class="line">        <span class="comment"># process each item in the batch</span></span><br><span class="line">        <span class="keyword">if</span> lb == <span class="number">0.0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>):  <span class="comment"># dim H</span></span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>):  <span class="comment"># dim W</span></span><br><span class="line"></span><br><span class="line">                loc_map[item_i, <span class="number">0</span>, y, x] = <span class="built_in">float</span>(x) - coord[<span class="number">0</span>]  <span class="comment"># dist_xt</span></span><br><span class="line">                loc_map[item_i, <span class="number">1</span>, y, x] = <span class="built_in">float</span>(y) - coord[<span class="number">1</span>]  <span class="comment"># dist_yt</span></span><br><span class="line">                loc_map[item_i, <span class="number">2</span>, y, x] = <span class="built_in">float</span>(x) - coord[<span class="number">2</span>]  <span class="comment"># dist_xb</span></span><br><span class="line">                loc_map[item_i, <span class="number">3</span>, y, x] = <span class="built_in">float</span>(y) - coord[<span class="number">3</span>]  <span class="comment"># dist_yb</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loc_map</span><br></pre></td></tr></table></figure>
<h3 id="4-结合关键点检测增强检测效果"><a href="#4-结合关键点检测增强检测效果" class="headerlink" title="4. 结合关键点检测增强检测效果"></a>4. 结合关键点检测增强检测效果</h3><p><img src="../images/OD-DenseBox%E9%9A%8F%E8%AE%B0/image-20220124195107507.png" alt="image-20220124195107507" style="zoom: 80%;" /></p>
<ul>
<li>DenseBox加入关键点检测的任务分支时模型的精度会进一步提升，这时只需要在图3的conv3_4和conv4_4融合之后的结果上添加一个用于关键点检测的分支即可，分支的详细结构如Figure4所示。假设样本有<script type="math/tex">N</script>个关键点（在MALF中 72 ），DenseBox的关键点检测的输出是 <script type="math/tex">N</script>个热图，热图中的每个像素点表示该点对应位置关键点的置信度。</li>
<li>加入关键点检测分支之后，DenseBox根据关键点的置信度图和boudning box的置信度图构成了新的检测损失，并将其命名为Refine Network，通过拼接的方式融合了关键点检测的Conv5_2_landmark层和图2中bounding box的Conv5_2_det层，之后接了Max Pooling层，卷积层，上采样层最后生成新的预测值，这里也可以理解为新的score_map预测。</li>
</ul>
<p>加入关键点定位即Refine Network结构后，最终损失设计为：</p>
<p><img src="../images/OD-DenseBox%E9%9A%8F%E8%AE%B0/image-20220124195757989.png" alt="image-20220124195757989"></p>
<p>Paper中关于加入关键点定位的消融实验：</p>
<p><img src="../images/OD-DenseBox%E9%9A%8F%E8%AE%B0/image-20220124200516603.png" alt="image-20220124200516603" style="zoom:80%;" /></p>
<h3 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40221183">https://zhuanlan.zhihu.com/p/40221183</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44021975">https://zhuanlan.zhihu.com/p/44021975</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/244d7f1dcdab">https://www.jianshu.com/p/244d7f1dcdab</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/20/OCR-EAST%E9%9A%8F%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/20/OCR-EAST%E9%9A%8F%E8%AE%B0/" class="post-title-link" itemprop="url">OCR-EAST算法随记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-20 15:53:52 / 修改时间：17:30:58" itemprop="dateCreated datePublished" datetime="2022-01-20T15:53:52+08:00">2022-01-20</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="1-EAST"><a href="#1-EAST" class="headerlink" title="1. EAST"></a>1. EAST</h3><p>EAST: An Efficient and Accurate Scene Text Detector</p>
<p>paper :  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1704.03155.pdf">https://arxiv.org/pdf/1704.03155.pdf</a></p>
<p>code:</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/argman/EAST">https://github.com/argman/EAST</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/huoyijie/AdvancedEAST">https://github.com/huoyijie/AdvancedEAST</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/songdejia/EAST">https://github.com/songdejia/EAST</a></p>
</li>
</ul>
<p>​       </p>
<p>EAST算法整体框架如下图（e）所示，  第一个阶段是一个全卷积网络，结合了Unet的跨层特征聚合操作，直接输出文本框的预测，第二个阶段是对生成的文本预测框（旋转矩形或者矩形）通过NMS输出最终结果。</p>
<p><img src="../images/OCR-EAST%E9%9A%8F%E8%AE%B0/image-20220120160214048.png" alt="image-20220120160214048"></p>
<p>核心思想：</p>
<ul>
<li>提出两阶段的文本检测方法：全卷积网络(FCN)和非极大值抑制(NMS)，消除中间过程冗余，减少检测时间。</li>
<li>EAST可以检测单词级别，又可以检测文本行级别．检测的形状可以为任意形状的四边形。</li>
<li>采用了Locality-Aware NMS来对生成的几何进行过滤。</li>
</ul>
<h3 id="2-Pipeline"><a href="#2-Pipeline" class="headerlink" title="2. Pipeline"></a>2. Pipeline</h3><p>EAST结合了DenseBox和Unet网络中的特性，整体算法流程如下图所示：</p>
<p><img src="../images/OCR-EAST%E9%9A%8F%E8%AE%B0/image-20220120161817544.png" alt="image-20220120161817544"></p>
<pre><code>   1.       Feature Extractor : 采用通用的CNN网络，例如VGG,ResNet等作为主干网络，提取特征。
   2.       Feature-Merging:   采用类似Unet的结构，将主干网络不同level的特征图进行聚合，采用的是   UnPooling+Conv的结构，主要是解决文本行尺度变化大的问题。Early stage可以预测小的文本行，Late stage可以预测大的文本行。
   3.       Output:  网络输出主要包含文本得分和文本形状相关信息的预测信息，不同文本形状（RBOX、QUAD），网络输出也有区分。
            * 对于旋转框，输出文本得分图+ AABB boundingbox（相对于top、right、bottom、left）的偏移  + rotate angle旋转角度 （1 + 4 + 1）。
            * 对于矩形框，输出文本得分图+ 四个顶点相对于pixel location的坐标偏移 （1 + 8）。
</code></pre><p><img src="../images/OCR-EAST%E9%9A%8F%E8%AE%B0/image-20220120163514308.png" alt="image-20220120163514308"></p>
<h3 id="3-训练标签设置"><a href="#3-训练标签设置" class="headerlink" title="3. 训练标签设置"></a>3. 训练标签设置</h3><p><img src="../images/OCR-EAST%E9%9A%8F%E8%AE%B0/image-20220120163711384.png" alt="image-20220120163711384"></p>
<p>其中，RBOX的几何形状由4个通道的水平边界框（AABB）R和1个通道的旋转角度θ表示；AABB 4个通道分别表示从像素位置到矩形的顶部，右侧，底部，左侧边界的4个距离；QUAD使用8个数字来表示从矩形的四个顶点到像素位置的坐标偏移，由于每个距离偏移量都包含两个数字（Δxi;Δyi），因此几何形状输出包含8个通道。</p>
<p>score map 上QUAD的正面积为原图矩形区域的缩小版，如上图(a)(b)。</p>
<p>Box几何位置的确定，很多数据集(如ICDAR2015)是用QUAD的方式标注的，首先生成以最小面积覆盖区域的旋转矩形框。每个像素有一个正的分数值，我们计算它与文本框四边的距离，把它们放入四通道的RBOX 真值中， 如上图（c,d,e）。对于QUAD真值，8通道几何形状图每个像素的正分数值是它与四边形4个顶点的坐标偏移。</p>
<h3 id="4-Loss"><a href="#4-Loss" class="headerlink" title="4. Loss"></a>4. Loss</h3><p>loss由两部分构成，score map loss 和 geometry loss， 具体的参数含义请参考论文:</p>
<p><img src="../images/OCR-EAST%E9%9A%8F%E8%AE%B0/image-20220120165422237.png" alt="image-20220120165422237"></p>
<p>其中，分数图损失采用类平衡交叉熵损失，用于解决类别不平衡问题。</p>
<p><img src="../images/OCR-EAST%E9%9A%8F%E8%AE%B0/image-20220120165610853.png" alt="image-20220120165610853"></p>
<p>其中，几何参数损失分两种情况</p>
<ul>
<li><p>针对旋转几何参数，采用IOU损失</p>
<p><img src="../images/OCR-EAST%E9%9A%8F%E8%AE%B0/image-20220120165743212.png" alt="image-20220120165743212"></p>
</li>
<li><p>针对矩形几何参数，采用Smooth L1损失</p>
<p><img src="../images/OCR-EAST%E9%9A%8F%E8%AE%B0/image-20220120165832461.png" alt="image-20220120165832461"></p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/19/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/19/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/" class="post-title-link" itemprop="url">OCR-CTPN算法随记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-19 14:24:07" itemprop="dateCreated datePublished" datetime="2022-01-19T14:24:07+08:00">2022-01-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-20 09:45:39" itemprop="dateModified" datetime="2022-01-20T09:45:39+08:00">2022-01-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="CTPN文本检测算法"><a href="#CTPN文本检测算法" class="headerlink" title="CTPN文本检测算法"></a>CTPN文本检测算法</h3><p>不同于自然界的其他单独物体，例如行人，物品等，文本信息蕴含更多的序列信息，文本可以由”单个字符、字符的一部分、多个字符”组织成一个sequence，文本目标不像行人 或者物体这种普通目标具有独立，封闭的范围。所以在目标检测算法中融入循环神经网络（RNN, LSTM），利用上下文信息进行文本检测是一个不错的方法。</p>
<p>文本检测相较于一般的目标检测，主要有以下几种区别：</p>
<ul>
<li>文本信息边界不易确定， 例如单词内的空格与单词间的空格会导致文本的边界范围不清晰。</li>
<li>文本信息蕴含序列特征，上下文的序列信息有助于文本检测。</li>
<li>文本行的长度变化范围较大，相比如普通物体的尺度信息，普通检测算法难以生成质量好的Region Proposal(或者称为 Text Proposal)。</li>
</ul>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119144903230.png" alt="image-20220119144903230"></p>
<p>上图展示了用Faster-RCNN网络和CTPN的检测效果，可以看到通用的目标检测算法的检测框会偏移较多，且对文本信息的边界定义比较模糊，而CTPN算法要准确的多。</p>
<p>CTPN的整体架构如下图所示：</p>
<ul>
<li>采用VGG-16作为检测网络的BackBone，下采样16倍后在Conv5之后提取空间特征信息，输出维度为B × W × H × C；</li>
<li>在Conv5的特征图上进行3x3xC的卷积，输出维度仍为B × W × H × C，这一步中的每一个特征点都融合了周围3 × 3的信息。（原始论文中采用的caffe的img2col进行特征的Reshape，这里采用3x3xC卷积代替，pytorch/tensorflow框架下的实现）。</li>
<li>接着将B × W × H × C维度的特征Reshape为(BH) × W × C的特征，然后作为双向LSTM的输入提取每一行的的序列特征，最后输出特征维度为(BH) × W × 256，然后Reshape至B x 256 x H x W。</li>
<li>经过FC，输出B x H x W x 512。</li>
<li>N × H × W × 512 最后会经过一个类似RPN的网络，分成三个预测支路：如上图所示，其中一个分支输出N × H × W × 2k，这里的k指的是每个像素对应k个anchor，这里的2K指的是对某一个anchor的预测<script type="math/tex">v = [{v_c},{v_h}]</script>；第二个分支输出N × H × W × 2k，这里的2K指的是2K个前景背景得分，记做。最后一个分支输出N × H × W × k，这里是K个side-refinement，预测某个anchor预测<img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/equation.svg+xml" alt="[公式]"></li>
<li>RPN之后会输出类似下图b中的text proposal, 然后适用NMS进行过滤。</li>
<li>后处理，使用文本线构造方法合成一个完成的文本行，同时矫正倾斜的情况。</li>
</ul>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119144559264.png" alt="image-20220119144559264"></p>
<h3 id="双向LSTM"><a href="#双向LSTM" class="headerlink" title="双向LSTM"></a>双向LSTM</h3><p>VGG16提取的是空间特征，而LSTM学习的就是序列特征，而这里使用的是双向LSTM，更好的避免RNN当中的遗忘问题，更完整地提取出序列特征。</p>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119155253540.png" alt="image-20220119155253540"></p>
<h3 id="Text-Proposal生成"><a href="#Text-Proposal生成" class="headerlink" title="Text Proposal生成"></a>Text Proposal生成</h3><p>文本长度的剧烈变化是文本检测的挑战之一，作者认为文本在长度的变化比高度的变化剧烈得多，文本边界开始与结束的地方难以和Faster-rcnn一样去用anchor匹配回归，所以作者提出一种<strong>vertical anchor</strong>的方法，即<strong>我们只去预测文本的竖直方向上的位置，不去预测水平方向的位置，水平位置的确定只需要我们检测一个一个小的固定宽度的文本段，将他们对应的高度预测准确，最后再将他们连接在一起，就得到了我们的文本行</strong>，如下图所示：</p>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119151722196.png" alt="image-20220119151722196"></p>
<p>对于每个像素点设置的anchor的宽度都是固定的，为16像素（原图），对应到特征图上就为1个像素（下采用了16倍）。而高度则是从11到273变化，这里我们每个像素点取k=10个anchor。</p>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119163202430.png" alt="image-20220119163202430"></p>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119152255022.png" alt="image-20220119152255022" style="zoom:67%;" /></p>
<p>这样设置Anchors是为了：</p>
<ol>
<li>保证在 x方向上，Anchor覆盖原图每个点且不相互重叠。</li>
<li>不同文本在 y 方向上高度差距很大，所以设置Anchors高度为11-283，用于覆盖不同高度的文本目标。</li>
</ol>
<p>获得Anchor后，与Faster R-CNN类似，CTPN会做如下处理：</p>
<ol>
<li>Softmax判断Anchor中是否包含文本，即选出Softmax score大的正Anchor</li>
<li>Bounding box regression修正包含文本的Anchor的<strong>中心y坐标</strong>与<strong>高度</strong>。</li>
</ol>
<p>因为宽度是固定的，所以只需要anchor的中心的y坐标以及anchor的高度就可以确定一个anchor，其中带星号的为ground-truth，没有带星号的则是预测值，带a的则是对应anchor的值。</p>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119152533923.png" alt="image-20220119152533923"></p>
<h3 id="RPN层"><a href="#RPN层" class="headerlink" title="RPN层"></a>RPN层</h3><p>CTPN的RPN层和Faster R-CNN很像，</p>
<p>第一个分支输出的是anchor的位置，也就是anchor的两个参数，因为每个特征点配置10个anchor，所以这个分支的输出20个channel。</p>
<p>第二个分支则是输出前景背景的得分情况(text/non-text scores)，通过softmax计算得分，所以这里也是输出20个channel。</p>
<p>第三个分支则是输出最后水平精修side-refinement的比例o，这是由于每个anchor的宽是一定的，所以有时候会导致水平方向有一点不准，所以这时候就需要校准一下检测框。</p>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119155746789.png" alt="image-20220119155746789"></p>
<p>Xside标识检测框的左边界或者右边界，Cx 表示anchor中心的横坐标。Wa是anchor固定的宽度16个像素。可以把这个o理解为一个缩放的比例，来对最后的结果做一个准确的拉伸，下面这张图中红色的就是使用了side-refinement，黄色的则是没有使用的结果。</p>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119160052651.png" alt="image-20220119160052651" style="zoom:67%;" /></p>
<h3 id="文本线构造方法"><a href="#文本线构造方法" class="headerlink" title="文本线构造方法"></a>文本线构造方法</h3><p>经过RPN之后生成一串或者多串text proposal , 然后用文本线构造办法，把这些text proposal连接成一个文本检测框。</p>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119163644749.png" alt="image-20220119163644749" style="zoom: 67%;" /></p>
<p>为了说明问题，假设某张图有上图所示的2个text proposal，即蓝色和红色2组Anchor，CTPN采用如下算法构造文本线：</p>
<ol>
<li>按照水平 x 坐标排序Anchor</li>
<li>按照规则依次计算每个Anchor <script type="math/tex">bo{x_j}</script>的 <script type="math/tex">pair(bo{x_j})</script>，组成 <script type="math/tex">pair(bo{x_i},bo{x_j})</script></li>
<li>通过<script type="math/tex">pair(bo{x_i},bo{x_j})</script>建立一个Connect graph，最终获得文本检测框。</li>
</ol>
<p><strong>文本线构造算法通过如下方式建立每个Anchor</strong>  <script type="math/tex">bo{x_j}</script>的   <script type="math/tex">pair(bo{x_i},bo{x_j})</script><strong>：</strong></p>
<ul>
<li>正向寻找：<ul>
<li><ol>
<li>沿水平正方向，寻找和box_i水平距离小于50像素的候选Anchors（每个Anchor宽16像素，也就是最多正向找50/16=3个）</li>
<li>从候选Anchor中，挑出与box_i竖直方向<script type="math/tex">overla{p_v} > 0.7</script> 的Anchor</li>
<li>挑出符合条件2中Softmax Score最大的box_j</li>
</ol>
</li>
</ul>
</li>
<li>反向寻找<ul>
<li><ol>
<li>沿水平负方向，寻找和box_j水平距离小于50像素的候选Anchors</li>
<li>从候选Anchor中，挑出与box_j竖直方向<script type="math/tex">overla{p_v} > 0.7</script> 的Anchor</li>
<li>挑出符合条件2中Softmax Score最大的box_k</li>
</ol>
</li>
</ul>
</li>
<li>对比score_i 和 score_j:<ul>
<li><ol>
<li>如果<script type="math/tex">scor{e_i} >  = scor{e_k}</script> ,则说明这是一个长连接，则设置Graph(i,j) = True;</li>
<li>如果<script type="math/tex">scor{e_i} < scor{e_k}</script>, 则说明这不是一个最长的连接（即该连接肯定包含在另外一个更长的连接中）。</li>
</ol>
</li>
</ul>
</li>
</ul>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119165416880.png" alt="image-20220119165416880"></p>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>CTPN的损失函数如下图，分为三个部分：</p>
<p>(1) LS：每个anchor是否是正样本的classification loss</p>
<p>(2) Lv:每个anchor的中心y坐标和高度loss</p>
<p>(3) L0:文本区域两侧精修的x损失</p>
<p>和Faster-RCNN一样，以上的loss都采用smooth L1 loss。</p>
<p><img src="../images/OCR-CTPN%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/image-20220119165742503.png" alt="image-20220119165742503"></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/137540923">https://zhuanlan.zhihu.com/p/137540923</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34757009">https://zhuanlan.zhihu.com/p/34757009</a></li>
<li><a target="_blank" rel="noopener" href="https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/">https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/12/OCR%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90%E4%B9%8Btext-renderer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/12/OCR%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90%E4%B9%8Btext-renderer/" class="post-title-link" itemprop="url">OCR数据合成之text_renderer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-12 16:41:38" itemprop="dateCreated datePublished" datetime="2022-01-12T16:41:38+08:00">2022-01-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/11/OCR-%E5%B7%A5%E4%B8%9A%E5%AD%97%E7%AC%A6%E6%A3%80%E6%B5%8B%E5%AE%9E%E8%B7%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/11/OCR-%E5%B7%A5%E4%B8%9A%E5%AD%97%E7%AC%A6%E6%A3%80%E6%B5%8B%E5%AE%9E%E8%B7%B5/" class="post-title-link" itemprop="url">OCR-工业字符检测实践</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-11 11:24:21" itemprop="dateCreated datePublished" datetime="2022-01-11T11:24:21+08:00">2022-01-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-24 17:49:14" itemprop="dateModified" datetime="2022-01-24T17:49:14+08:00">2022-01-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="1-PaddleOCR"><a href="#1-PaddleOCR" class="headerlink" title="1. PaddleOCR"></a>1. PaddleOCR</h3><ul>
<li>使用PPOCRLabel标注，完成自定义训练数据集的准备；</li>
<li>训练文本检测模型；</li>
<li>训练文本识别模型；</li>
<li>训练模型转换为inference模型；</li>
<li>基于python引擎的PP-OCR模型推理预测，串联检测+识别。</li>
</ul>
<h3 id="2-PPOCRLabel"><a href="#2-PPOCRLabel" class="headerlink" title="2. PPOCRLabel"></a>2. PPOCRLabel</h3><p>PPOCRLabel是一款适用于OCR领域的半自动化图形标注工具，内置PP-OCR模型对数据自动标注和重新识别。使用Python3和PyQT5编写，支持矩形框标注和四点标注模式，导出格式可直接用于PaddleOCR检测和识别模型的训练。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.4/PPOCRLabel/README_ch.md">https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.4/PPOCRLabel/README_ch.md</a></li>
</ul>
<p>使用步骤：</p>
<ul>
<li>cd ./PPOCRLabel  </li>
<li>python .\PPOCRLabel.py —lang ch</li>
<li>打开待标注文件夹，加载标注图像</li>
<li>勾选 文件/自动导出标记结果</li>
<li>标注</li>
<li>导出识别结果（切小图）</li>
</ul>
<h3 id="3-训练文本检测模型"><a href="#3-训练文本检测模型" class="headerlink" title="3. 训练文本检测模型"></a>3. 训练文本检测模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单机单卡训练 mv3_db 模型</span></span><br><span class="line">python3 tools/train.py -c configs/det/det_mv3_db.yml \</span><br><span class="line">     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单机多卡训练，通过 --gpus 参数设置使用的GPU ID</span></span><br><span class="line">python3 -m paddle.distributed.launch --gpus <span class="string">&#x27;0,1,2,3&#x27;</span> tools/train.py -c configs/det/det_mv3_db.yml \</span><br><span class="line">     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多机多卡训练，通过 --ips 参数设置使用的机器IP地址，通过 --gpus 参数设置使用的GPU ID</span></span><br><span class="line">python3 -m paddle.distributed.launch --ips=<span class="string">&quot;xx.xx.xx.xx,xx.xx.xx.xx&quot;</span> --gpus <span class="string">&#x27;0,1,2,3&#x27;</span> tools/train.py -c configs/det/det_mv3_db.yml \</span><br><span class="line">     -o Global.pretrained_model=./pretrain_models/MobileNetV3_large_x0_5_pretrained</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="4-训练文本识别模型"><a href="#4-训练文本识别模型" class="headerlink" title="4. 训练文本识别模型"></a>4. 训练文本识别模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPU训练 支持单卡，多卡训练</span></span><br><span class="line"><span class="comment"># 训练icdar15英文数据 训练日志会自动保存为 &quot;&#123;save_model_dir&#125;&quot; 下的train.log</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#单卡训练（训练周期长，不建议）</span></span><br><span class="line">python3 tools/train.py -c configs/rec/rec_icdar15_train.yml</span><br><span class="line"></span><br><span class="line"><span class="comment">#多卡训练，通过--gpus参数指定卡号</span></span><br><span class="line">python3 -m paddle.distributed.launch --gpus <span class="string">&#x27;0,1,2,3&#x27;</span>  tools/train.py -c configs/rec/rec_icdar15_train.yml</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="5-转换为inference模型部署"><a href="#5-转换为inference模型部署" class="headerlink" title="5. 转换为inference模型部署"></a>5. 转换为inference模型部署</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1. 文本检测模型转inference模型</span></span><br><span class="line">python tools/export_model.py -c configs/det/det_mv3_db.yml -o Global.pretrained_model=<span class="string">&quot;./output/sense_ocr_det/best_accuracy&quot;</span> Global.save_inference_dir=<span class="string">&quot;./sense_model/sense_det/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2. 文本识别模型转inference模型</span></span><br><span class="line">python tools/export_model.py -c configs/rec/rec_icdar15_train.yml  -o Global.pretrained_model=./output/sense_ocr_rec/best_accuracy  Global.save_inference_dir=./sense_model/sense_rec/</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="6-联合文本检测-文本识别测试"><a href="#6-联合文本检测-文本识别测试" class="headerlink" title="6. 联合文本检测+文本识别测试"></a>6. 联合文本检测+文本识别测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 tools/infer/predict_system.py --image_dir=<span class="string">&quot;./doc/imgs/00018069.jpg&quot;</span> --det_model_dir=<span class="string">&quot;./sense_model/sense_det/&quot;</span> --rec_model_dir=<span class="string">&quot;./sense_model/sense_rec/&quot;</span> --use_angle_cls=false</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="../images/OCR-%E5%B7%A5%E4%B8%9A%E5%AD%97%E7%AC%A6%E6%A3%80%E6%B5%8B%E5%AE%9E%E8%B7%B5/image-20220124174811464.png" alt="image-20220124174811464" style="zoom:80%;" /></p>
<p><img src="../images/OCR-%E5%B7%A5%E4%B8%9A%E5%AD%97%E7%AC%A6%E6%A3%80%E6%B5%8B%E5%AE%9E%E8%B7%B5/image-20220124174909037.png" alt="image-20220124174909037" style="zoom:80%;" /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/07/Bounding-Box-Regression-Loss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/07/Bounding-Box-Regression-Loss/" class="post-title-link" itemprop="url">Bounding Box Regression Loss</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-07 14:36:31 / 修改时间：14:49:12" itemprop="dateCreated datePublished" datetime="2022-01-07T14:36:31+08:00">2022-01-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="1"><a href="#1" class="headerlink" title="### 1."></a>### 1.</h3><p>边界框回归的三大几何因素：重叠面积、中心点距离、纵横比</p>
<ul>
<li>IOU Loss：考虑了重叠面积，归一化坐标尺度</li>
<li>GIOU Loss：考虑了重叠面积，基于IOU解决边界框不相交时loss等于0的问题</li>
<li>DIOU Loss：考虑了重叠面积和中心点距离，基于IOU解决GIOU收敛慢的问题</li>
<li>CIOU Loss：考虑了重叠面积、中心点距离、纵横比，基于DIOU提升回归精确度</li>
<li>EIOU Loss：考虑了重叠面积，中心点距离、长宽边长真实差，基于CIOU解决了纵横比的模糊定义，并添加Focal Loss解决BBox回归中的样本不平衡问题</li>
</ul>
<p><img src="../images/Bounding-Box-Regression-Loss/image-20220107144907598.png" alt="image-20220107144907598" style="zoom:80%;" /></p>
<h3 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/270663039">https://zhuanlan.zhihu.com/p/270663039</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/amusi1994/article/details/112966895">https://blog.csdn.net/amusi1994/article/details/112966895</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/375745293">https://zhuanlan.zhihu.com/p/375745293</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/06/RoI-Pooling-%E4%B8%8E-RoI-Align/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/06/RoI-Pooling-%E4%B8%8E-RoI-Align/" class="post-title-link" itemprop="url">RoI Pooling, RoI Align, PS-RoI Pooling</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-06 10:21:38 / 修改时间：14:31:20" itemprop="dateCreated datePublished" datetime="2022-01-06T10:21:38+08:00">2022-01-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="1-RoI"><a href="#1-RoI" class="headerlink" title="1. RoI"></a>1. RoI</h3><p>RoI(Region of Interest), 从原图中通过某些区域选择方法得到的候选区域。</p>
<p>量化（quantization）是指将输入连续值（或者大量可能的离散值）采样为有限多个离散值的过程。或者理解为，将输入数据集（如实数）约束到离散集（如整数）的过程。</p>
<p>RoI Pooling 和 RoI Align 均是将任意大小的特征图（输入），映射为固定尺寸的特征（输出）。</p>
<h3 id="2-RoI-Pooling"><a href="#2-RoI-Pooling" class="headerlink" title="2. RoI Pooling"></a>2. RoI Pooling</h3><p><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106102835274.png" alt="image-20220106102835274" style="zoom:50%;" /></p>
<p>RoI Pooling的作用本质上是为了将不同尺寸的RoI特征转换为相同的特征图输出，保证特征图展开（flatten）后具有相同的大小尺寸，能够与下层的全连接层连接，分别执行线性分类(linear classifier)和边框回归(bounding box regressor)</p>
<ul>
<li><p>1 将RoI 对齐到特征图的网格单元（snap to grid cell）</p>
<p>首先将特征图上的浮点数RoI（举例 ： 665 <em> 665 —&gt; 665/32=20.78 —&gt;  20.78 </em> 20.78 —&gt; 20 <em> 20）进行量化。下图中绿色框为RoI对应的实际区域（<em>*由于经过特征尺度变换，导致RoI的坐标会可能会落到特征图的单元之间</em></em>）， 蓝色框代表量化(网格对齐)后的RoI所对应的特征图。</p>
</li>
</ul>
<p><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106103613312.png" alt="image-20220106103613312" style="zoom:50%;" /></p>
<p><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106105319288.png" alt="image-20220106105319288" style="zoom:67%;" /></p>
<ul>
<li><p>2 划分网格为子区域</p>
<p>将上一步得到的量化RoI 特征进一步细分为量化的空间单元(bin)。为了得到输出的特征图为 2 <em> 2 </em> 512 ，这里的量化操作就是将上一步的到量化特征图划分为2 <em> 2个特征单元。如果无法通过直接均分得到量化的子区域，通过分别采取向上取整（ceil）和向下取整（floor）的到对应的单元尺寸大小。以当前 4 </em> 5 尺寸的特征图为例，对于宽度方向 4 / 2 = 2， 对于高度方向 5 /  2 = 2.5)， 通过向上和向下取整整，确定高度方向特征子区域的大小分别为2和3。</p>
</li>
</ul>
<p><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106110344846.png" alt="image-20220106110344846" style="zoom: 67%;" /></p>
<ul>
<li>3 子区域最大池化</li>
</ul>
<p>在每一个子区域执行聚合操作得到单元的特征值（一般是最大池化）。对上一步得到的 2 <em> 2个子区域分别做最大池化操作，得到 2 </em> 2 * 512的目标特征图。</p>
<p>缺陷</p>
<p>每一次量化操作都会对应着轻微的区域特征错位（misaligned）， 这些量化操作在RoI和提取到的特征之间引入了偏差。这些量化可能不会影响对分类任务，但它对预测像素精度掩模有很大的负面影响。</p>
<h3 id="3-RoI-Align"><a href="#3-RoI-Align" class="headerlink" title="3. RoI Align"></a>3. RoI Align</h3><p>RoI Align在Mask RCNN中被首次提出，是针对RoI Pooling 在语义分割等精细度任务中的精确度的问题提出的改进方案。</p>
<ul>
<li>1 遍历候选每个候选区域，保持浮点数边界不做量化（不对齐网格单元）；同时平均分网格分为 2 * 2个子网格区域，每个单元的边界也不做量化。</li>
</ul>
<p><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106110933568.png" alt="image-20220106110933568" style="zoom:67%;" /></p>
<ul>
<li><p>2 对于每个区域选择4个规则采样点（分别对应将区域进一步平均分为四个区域，取每个子区域的中点)。</p>
<p><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106111115211.png" alt="image-20220106111115211" style="zoom:67%;" /></p>
</li>
<li><p>3 利用双线性插值计算得到四个采用点的像素值大小。下图为一个规则采样点所对应的邻近区域示意图。</p>
</li>
</ul>
<p><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106111143717.png" alt="image-20220106111143717" style="zoom:67%;" /></p>
<ul>
<li>4 利用最大池化（max pooling）或平均池化(average pooling)分别对每个子区域执行聚合操作，得到最终的特征图。</li>
</ul>
<p><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106111213485.png" alt="image-20220106111213485" style="zoom:67%;" /></p>
<p>下图 绿色表示ROI区域额外信息， 蓝色(第一次量化)和天蓝色(第二次量化)表示丢失信息</p>
<p><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106112934397.png" alt="image-20220106112934397" style="zoom:80%;" /></p>
<h3 id="4-PS-RoI-Pooling"><a href="#4-PS-RoI-Pooling" class="headerlink" title="4. PS-RoI Pooling"></a>4. PS-RoI Pooling</h3><p>位置敏感池化，RFCN引入位置敏感池化，主要基于以下两方面原因</p>
<ol>
<li>引入位置敏感，卷积可以保持位置信息，但是经过全连接后，位置信息不在保留。</li>
<li>对于region-based方法，通常会分为几个sub-network, 一是提取图像特征的主干网络，与region无关，各region共享，计算量大。二是生成候选区域的RPN网络，三是用来分类和回归的prediction网络， 每个region会单独执行这部分sub-network。而RFCN将计算量大卷积尽可能地移到共享的主干网络中，最后仅使用一层卷积做prediction，减少了计算量。</li>
</ol>
<p>为了实现位置敏感就提出PS-ROI Pooling，核心思想是position sensitive score map。<br>把位置信息以层的形式就组成position sensitive score maps，进行一次卷积就计算了多个ROI的最终输出(固定长度)。</p>
<p><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106135532044.png" alt="image-20220106135532044" style="zoom:80%;" /></p>
<ul>
<li>首先，在共享特征图之后添加1 <em> 1 </em> k^2(c+1)维的卷积形成位置敏感特征图，然后在位置敏感特征图上进行PS-RoI Pooling。 k^2代表的是RoI中划分的位置区域数目。比如k=3, 即代表上左（左上角），上中，上右，中左，中中，中右，下左，下中，下右（右下角）共9个子区域。 c+1 代表所有类别加上背景。 k^2(c+1)张特征图每c+1张分成一组，共包含k^2组，每一组负责其对应区域的响应，如上图所示。</li>
<li>然后进行PS-RoI Pooling，对位置敏感特征图上的RoI区域划分子区域（k^2）, 每个对应位置（c+1）内进行全局平均池化， 最后获得一组（c+1）* k^2的投票矩阵。</li>
<li>最后，每个类对应有9个位置的投票值，这9个值求和，就是这个类的概率。</li>
</ul>
<h3 id="5-RoI-Align的反向传播"><a href="#5-RoI-Align的反向传播" class="headerlink" title="5. RoI Align的反向传播"></a>5. RoI Align的反向传播</h3><p>和ROI Pooling核心思想是一样的，但是在ROI Align中，i ∗ ( r , j )是一个浮点数的坐标位置(前向传播时计算出来的采样点)，在池化前的特征图中，每一个与 i ∗ ( r , j ) 横纵坐标均小于1的点都应该接受与此对应的点y(r，j)回传的梯度，故ROI Align 的反向传播公式如下:<br><img src="../images/RoI-Pooling-%E4%B8%8E-RoI-Align/image-20220106142854108.png" alt="image-20220106142854108"></p>
<p>上式中，d(.)表示两点之间的距离，Δ h 和Δ w 表示 i 与 i ∗ ( r , j )横纵坐标的差值，这里作为双线性内插的系数乘在原始的梯度上。</p>
<h3 id="6-Reference"><a href="#6-Reference" class="headerlink" title="6 . Reference"></a>6 . Reference</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/161540817">https://zhuanlan.zhihu.com/p/161540817</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1689064">https://cloud.tencent.com/developer/article/1689064</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/04/%E5%9B%BE%E5%83%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/04/%E5%9B%BE%E5%83%8F%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F/" class="post-title-link" itemprop="url">图像相似度计算方式</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-04 10:32:37" itemprop="dateCreated datePublished" datetime="2022-01-04T10:32:37+08:00">2022-01-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/28/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/28/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/" class="post-title-link" itemprop="url">目标检测中的FPN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-28 17:17:24 / 修改时间：18:07:42" itemprop="dateCreated datePublished" datetime="2021-12-28T17:17:24+08:00">2021-12-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/" itemprop="url" rel="index"><span itemprop="name">object detection</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/ssl/" itemprop="url" rel="index"><span itemprop="name">ssl</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/ssl/image-classification/" itemprop="url" rel="index"><span itemprop="name">image classification</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>z 摘录自小纸屑 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148738276">https://zhuanlan.zhihu.com/p/148738276</a></p>
<h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p>FPN结构，特征金字塔网络，主要用作不同尺度的特征融合，从而提高目标检测算法的精度。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228172104469.png" alt="image-20211228172104469"></p>
<p>常见的物体检测算法，其实可以分解为三个递进的阶段：</p>
<ol>
<li><p>Backbone特征提取阶段</p>
<p>Backbone生成的特征，一般按stage划分，分别记作C1、C2、C3、C4、C5、C6、C7等，其中的数字与stage的编号相同，代表的是分辨率减半的次数，如C2代表stage2输出的特征图，分辨率为输入图片的1/4，C5代表，stage5输出的特征图，分辨率为输入图片的1/32。</p>
</li>
<li><p>Neck特征融合阶段</p>
<p>FPN一般将上一步生成的不同分辨率特征作为输入，输出经过融合后的特征。输出的特征一般以P作为编号标记。如FPN的输入是，C2、C3、C4、C5、C6，经过融合后，输出为P2、P3、P4、P5、P6。这个过程可以用数学公式表达：</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228172506926.png" alt="image-20211228172506926"></p>
</li>
<li><p>Head检测阶段</p>
<p>FPN输出融合后的特征后，就可以输入到检测头做具体的物体检测。</p>
</li>
</ol>
<h3 id="2-FPN及其变种结构"><a href="#2-FPN及其变种结构" class="headerlink" title="2. FPN及其变种结构"></a>2. FPN及其变种结构</h3><p>物体检测性能提升，一般主要通过数据增强、改进Backbone、改进FPN、改进检测头、改进loss、改进后处理等6个常用手段。其中FPN自从被提出来，先后迭代了不少版本。大致迭代路径如下图：</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228172706353.png" alt="image-20211228172706353"></p>
<ol>
<li>无融合  </li>
</ol>
<p>无融合，又利用多尺度特征的典型代表SSD，它直接利用不同stage的特征图分别负责不同scale大小物体的检测。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228172812278.png" alt="image-20211228172812278"></p>
<ol>
<li><p>自上而下单向融合</p>
<p>自上而下单向融合的FPN，事实上仍然是当前物体检测模型的主流融合模式。如我们常见的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.01497">Faster RCNN</a>、<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.06870">Mask RCNN</a>、<a href="https://link.zhihu.com/?target=https%3A//pjreddie.com/media/files/papers/YOLOv3.pdf">Yolov3</a>、<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.02002">RetinaNet</a>、<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1712.00726">Cascade RCNN</a>等，具体各个FPN的内部细节如下图。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228172915560.png" alt="image-20211228172915560"></p>
</li>
</ol>
<p><strong>a）Faster/Master/Cascade RCNN中的FPN</strong></p>
<p>Faster/Master/Cascade RCNN中的FPN，利用了C2-C6五个stage的特征，其中C6是从C5直接施加1x1/2的MaxPooling操作得到。FPN融合后得到P2-P6，其中P6直接等于C6，P5是先经过1x1Conv，再经过3x3Conv得到，P2-P4均是先经过1x1Conv，再融合上一层2xUpsample的特征，再经过3x3Conv得到。具体过程可以看上图。</p>
<p><strong>b）RetinaNet中的FPN</strong></p>
<p>RetinaNet中的FPN，利用了C3-C7五个stage的特征，其中C6是从C5直接施加3x3/2的Conv操作得到，C7是从C6直接施加3x3/2的Conv操作得到。FPN融合后得到P3-P7，其中P6、P7直接等于C6、C7，P5是先经过1x1Conv，再经过3x3Conv得到，P3-P4均是先经过1x1Conv，再融合上一层2xUpsample的特征，再经过3x3Conv得到。具体过程可以看上图。</p>
<p>可以看出，RetinaNet基本与Faster/Master/Cascade RCNN中的FPN一脉相承。只是利用的stage的特征略有差别，Faster/Master/Cascade RCNN利用了高分辨率低语义的C2，RetinaNet利用了更低分辨率更高语义的C7。其他都是细微的差别。</p>
<p><strong>c）Yolov3中的FPN</strong></p>
<p>Yolov3中的FPN与上述两个有比较大的区别。首先，Yolov3中的FPN只利用到了C3-C5三个stage的特征；其次，从C5征到P5特征，会先经过5层Conv，然后再经过一层3x3Conv；最后，C3-C4到P3-P4特征，上一层特征会先经过1x1Conv+2xUpsample，然后先与本层特征concatenate，再经过5层Conv，之后经过一层3x3Conv。</p>
<ol>
<li>简单双向融合</li>
</ol>
<p>FPN自从提出来以后，均是只有从上向下的融合，<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.01534">PANet</a>是第一个提出从下向上二次融合的模型，并且PANet就是在Faster/Master/Cascade RCNN中的FPN的基础上，简单增了从下而上的融合路径。看下图。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173101063.png" alt="image-20211228173101063"></p>
<ol>
<li>复杂的双向融合</li>
</ol>
<p>PANet的提出证明了双向融合的有效性，而PANet的双向融合较为简单，因此不少文章在FPN的方向上更进一步，尝试了更复杂的双向融合，如<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1911.09516">ASFF</a>、<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.07392">NAS-FPN</a>和<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1911.09070">BiFPN</a>。</p>
<p><strong>ASFF</strong><br>ASFF（论文：Learning Spatial Fusion for Single-Shot Object Detection）作者在YOLOV3的FPN的基础上，研究了每一个stage再次融合三个stage特征的效果。如下图。其中不同stage特征的融合，采用了注意力机制，这样就可以控制其他stage对本stage特征的贡献度。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173150634.png" alt="image-20211228173150634"></p>
<p><strong>NAS-FPN和BiFPN</strong><br>NAS-FPN和BiFPN，都是google出品，思路也一脉相承，都是在FPN中寻找一个有效的block，然后重复叠加，这样就可以弹性的控制FPN的大小。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173207992.png" alt="image-20211228173207992"></p>
<p>其中BiFPN的具体细节如下图。</p>
<p>改进点1，删除入度为1的节点，如果一个节点只有一个输入边且没有特征融合，那么它将对旨在融合不同特征的特征网络贡献较小。</p>
<p>改进点2，添加跳跃连接。如果原始输入和输出节点处于同一level，则在原始输入和输出节点之间添加一条额外的边，以便在不增加成本的情况下融合更多特征。</p>
<p>改进点3，将BiFPN视作一个基本单元，重复堆叠。不像PANet那样只有一个top-down和bottom-up路径，BiFPN将一对路径视为一个特征层，然后重复多次以得到更多高层特征融合。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173236688.png" alt="image-20211228173236688"></p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173943134.png" alt="image-20211228173943134"></p>
<ol>
<li><p><strong>Recursive-FPN</strong></p>
<p>Paper: <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2006.02334">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></p>
<p>递归FPN理解起来很容易，就是将传统FPN的融合后的输出，再输入给Backbone，进行二次循环，如下图。</p>
</li>
</ol>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173603992.png" alt="image-20211228173603992"></p>
<p>下图给出了FPN与Recursive-FPN的区别，并且把一个2层的递归FPN展开了，非常简单明了，不做过多介绍。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173616868.png" alt="image-20211228173616868"></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148738276">https://zhuanlan.zhihu.com/p/148738276</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/28/ASFF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/28/ASFF/" class="post-title-link" itemprop="url">ASFF</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-28 10:36:52" itemprop="dateCreated datePublished" datetime="2021-12-28T10:36:52+08:00">2021-12-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-31 15:04:44" itemprop="dateModified" datetime="2021-12-31T15:04:44+08:00">2021-12-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/" itemprop="url" rel="index"><span itemprop="name">object detection</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/ssl/" itemprop="url" rel="index"><span itemprop="name">ssl</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/ssl/image-classification/" itemprop="url" rel="index"><span itemprop="name">image classification</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="YOLOV3-ASFF"><a href="#YOLOV3-ASFF" class="headerlink" title="YOLOV3-ASFF"></a>YOLOV3-ASFF</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p>  YOLOV3-ASFF为了解决FPN不同特征尺度之间的不一致问题，提出自适应空间特征融合策略。它通过设置可学习权重因子对不同尺度的特征进行自适应(可学习)融合，通过在空间上过滤冲突信息从而抑制梯度反传时的不一致问题，从而改善了特征的比例不变性，也降低了时间开销。</p>
<p><img src="../images/ASFF/image-20211228151839649.png" alt="image-20211228151839649"></p>
<p>除了自适应空间特征融合，YOLOv3-ASFF在YOLOv3基础上博采众长，集合了MixUp数据增强，学习率cosine衰减策略，异步BN，Guided Anchoring，回归loss改为IoU loss等一系列tricks。 其strong yolov3-608 在COCO2017上达到了 38.8AP + 50fps的效果， 超过了原始YOLOv3-608 ： 33.0AP + 53fps。</p>
<blockquote>
<p>论文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.09516v2.pdf">https://arxiv.org/pdf/1911.09516v2.pdf</a></p>
<p>code: <a target="_blank" rel="noopener" href="https://github.com/GOATmessi7/ASFF">https://github.com/GOATmessi7/ASFF</a></p>
</blockquote>
<h3 id="2-自适应空间特征融合（ASFF）"><a href="#2-自适应空间特征融合（ASFF）" class="headerlink" title="2.自适应空间特征融合（ASFF）"></a>2.自适应空间特征融合（ASFF）</h3><p><img src="../images/ASFF/image-20211228152822280.png" alt="image-20211228152822280"></p>
<p>在上图中， 绿色框里代表一个ASFF模块，X1, X2, X3代表level1, level2, level3三层的特征图，<script type="math/tex">\alpha, \beta, \gamma</script> 则为可学习的权重参数。 加权融合后特征最为Head层的输入。</p>
<p><img src="../images/ASFF/image-20211228153257070.png" alt="image-20211228153257070"></p>
<p>以ASFF-2举例说明：</p>
<ol>
<li>对Level1的特整图（1，512，10，10）压缩通道数， 然后插值后变为（1，256，20，20）的特征向量。</li>
<li>对Level3的特征图进行<script type="math/tex">k=3*3，s=2</script>卷积，输出特征图（1，256，20，20）</li>
<li>Level2层级上特征图不需改变，然后对三个层级的特征图进行<script type="math/tex">k=1*1，s=1</script>的卷积，得到三个层级的空间权重向量（1，16*3， 20，20）</li>
<li>对空间特征降维，并沿通道方向进行softmax操作，得到（1，3，20，20）的权重向量。</li>
<li>特征融合。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ASFF</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, level, rfb=<span class="literal">False</span>, vis=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ASFF, self).__init__()</span><br><span class="line">        self.level = level</span><br><span class="line">        self.dim = [<span class="number">512</span>, <span class="number">256</span>, <span class="number">256</span>]</span><br><span class="line">        self.inter_dim = self.dim[self.level]</span><br><span class="line">        <span class="keyword">if</span> level==<span class="number">0</span>:</span><br><span class="line">            self.stride_level_1 = add_conv(<span class="number">256</span>, self.inter_dim, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">            self.stride_level_2 = add_conv(<span class="number">256</span>, self.inter_dim, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">            self.expand = add_conv(self.inter_dim, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> level==<span class="number">1</span>:</span><br><span class="line">            self.compress_level_0 = add_conv(<span class="number">512</span>, self.inter_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            self.stride_level_2 = add_conv(<span class="number">256</span>, self.inter_dim, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">            self.expand = add_conv(self.inter_dim, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> level==<span class="number">2</span>:</span><br><span class="line">            self.compress_level_0 = add_conv(<span class="number">512</span>, self.inter_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            self.expand = add_conv(self.inter_dim, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        compress_c = <span class="number">8</span> <span class="keyword">if</span> rfb <span class="keyword">else</span> <span class="number">16</span>  <span class="comment">#when adding rfb, we use half number of channels to save memory</span></span><br><span class="line"></span><br><span class="line">        self.weight_level_0 = add_conv(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.weight_level_1 = add_conv(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.weight_level_2 = add_conv(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.weight_levels = nn.Conv2d(compress_c*<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.vis= vis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x_level_0, x_level_1, x_level_2</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.level==<span class="number">0</span>:</span><br><span class="line">            level_0_resized = x_level_0</span><br><span class="line">            level_1_resized = self.stride_level_1(x_level_1)</span><br><span class="line"></span><br><span class="line">            level_2_downsampled_inter =F.max_pool2d(x_level_2, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">            level_2_resized = self.stride_level_2(level_2_downsampled_inter)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.level==<span class="number">1</span>:</span><br><span class="line">            level_0_compressed = self.compress_level_0(x_level_0)    <span class="comment"># [1,512,10,10] --&gt;  [1,256,10,10] </span></span><br><span class="line">            level_0_resized =F.interpolate(level_0_compressed, scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;nearest&#x27;</span>) <span class="comment"># [1,256,20,20]</span></span><br><span class="line">            level_1_resized =x_level_1 <span class="comment"># [1,256,20,20]</span></span><br><span class="line">            level_2_resized =self.stride_level_2(x_level_2) <span class="comment">#[1,256,40,40] --&gt; [1,256, 20,20]</span></span><br><span class="line">        <span class="keyword">elif</span> self.level==<span class="number">2</span>:</span><br><span class="line">            level_0_compressed = self.compress_level_0(x_level_0)</span><br><span class="line">            level_0_resized =F.interpolate(level_0_compressed, scale_factor=<span class="number">4</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">            level_1_resized =F.interpolate(x_level_1, scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">            level_2_resized =x_level_2</span><br><span class="line"></span><br><span class="line">        level_0_weight_v = self.weight_level_0(level_0_resized)  <span class="comment">#[1,16,20,20]</span></span><br><span class="line">        level_1_weight_v = self.weight_level_1(level_1_resized)  <span class="comment">#[1,16,20,20]</span></span><br><span class="line">        level_2_weight_v = self.weight_level_2(level_2_resized)  <span class="comment">#[1,16,20,20]</span></span><br><span class="line">        levels_weight_v = torch.cat((level_0_weight_v, level_1_weight_v, level_2_weight_v),<span class="number">1</span>)  <span class="comment">#[1,48,20,20]</span></span><br><span class="line">        levels_weight = self.weight_levels(levels_weight_v)  <span class="comment"># [1,3,20,20]</span></span><br><span class="line">        </span><br><span class="line">        levels_weight = F.softmax(levels_weight, dim=<span class="number">1</span>)  </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#softmax激活函数对三个通道的各位置进行运算，对不同通道的特征赋予不同的权重（归一化后）,以达到自适应（可学习）的特征融合</span></span><br><span class="line">        fused_out_reduced = level_0_resized * levels_weight[:,<span class="number">0</span>:<span class="number">1</span>,:,:]+\</span><br><span class="line">                            level_1_resized * levels_weight[:,<span class="number">1</span>:<span class="number">2</span>,:,:]+\</span><br><span class="line">                            level_2_resized * levels_weight[:,<span class="number">2</span>:,:,:]</span><br><span class="line"></span><br><span class="line">        out = self.expand(fused_out_reduced)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.vis:</span><br><span class="line">            <span class="keyword">return</span> out, levels_weight, fused_out_reduced.<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = ASFF(level=<span class="number">1</span>)</span><br><span class="line">    l1 = torch.ones(<span class="number">1</span>,<span class="number">512</span>,<span class="number">10</span>,<span class="number">10</span>) <span class="comment">#  FPN --&gt; L1</span></span><br><span class="line">    l2 = torch.ones(<span class="number">1</span>,<span class="number">256</span>,<span class="number">20</span>,<span class="number">20</span>) <span class="comment">#  FPN --&gt; L2</span></span><br><span class="line">    l3 = torch.ones(<span class="number">1</span>,<span class="number">256</span>,<span class="number">40</span>,<span class="number">40</span>) <span class="comment">#  FPN --&gt; L3</span></span><br><span class="line"></span><br><span class="line">    out = model(l1,l2,l3)</span><br><span class="line">    <span class="built_in">print</span>(out.shape) <span class="comment">#[1,256,20,20]</span></span><br></pre></td></tr></table></figure>
<h3 id="3-ASFF的可解释性"><a href="#3-ASFF的可解释性" class="headerlink" title="3. ASFF的可解释性"></a>3. ASFF的可解释性</h3><p>以YOLOv3为例，加入FPN后通过链式法则在backward的时候梯度是这样计算的：</p>
<p><img src="../images/ASFF/image-20211228154404525.png" alt="image-20211228154404525"></p>
<p>通常情况下，特征图尺寸增大通过插值（interpolation）实现，特征图尺寸缩小通过pooling来实现，假设：</p>
<p><img src="../images/ASFF/image-20211228154629987.png" alt="image-20211228154629987"></p>
<p>对于融合运算(add, concat)，相当于对输出特征的activation操作，其导数也将为固定值，可以将它的值同样简化为1。</p>
<p><img src="../images/ASFF/image-20211228154848675.png" alt="image-20211228154848675"><img src="../images/ASFF/image-20211228154856380.png" alt="image-20211228154856380"></p>
<p>最终简化为：</p>
<p><img src="../images/ASFF/image-20211228155128091.png" alt="image-20211228155128091"></p>
<p>若Level1上的某位置上为正样本特征，那如果Level2, Level3上对应位置却为负样本，那么在反向传播的梯度中既包含正样本信息也包含负样本信息，就会造成信息的不一致性，从而降低Level1上各个特征的学习效率。而通过ASFF的方式，反向传播的梯度表达式就变成了：</p>
<p><img src="../images/ASFF/image-20211228155556739.png" alt="image-20211228155556739"></p>
<p>因此可以动态地学习权重参数，进行更优的特征融合。</p>
<p><img src="../images/ASFF/image-20211228155840045.png" alt="image-20211228155840045"></p>
<p>上图可视化的结果进一步阐明了ASFF的有效性。比如大尺度的目标，可以看到斑马实际上是在level1这个特征图上被检测到的，并且观察level1这一层的权重信息<script type="math/tex">\alpha, \beta, \gamma</script>可以发现，对于图中斑马这种大目标更容易被高层的特征捕捉到，因为对于大物体需要更大的感受野和高级语义特征。而对于小目标的检测，可以看到羊更多的是被level2和level3检测到，这也说明了对于小物体更需要底层特征中的细粒度特征来检测。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><a target="_blank" rel="noopener" href="https://qiyuan-z.github.io/2020/04/20/%E8%87%AA%E9%80%82%E5%BA%94%E7%A9%BA%E9%97%B4%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88-(ASFF)/">https://qiyuan-z.github.io/2020/04/20/%E8%87%AA%E9%80%82%E5%BA%94%E7%A9%BA%E9%97%B4%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88-(ASFF)/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wujianming-110117/p/12921308.html">https://www.cnblogs.com/wujianming-110117/p/12921308.html</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/129363523">https://zhuanlan.zhihu.com/p/129363523</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/138816612">https://zhuanlan.zhihu.com/p/138816612</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">wyukai</p>
  <div class="site-description" itemprop="description">do</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wyukai</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
