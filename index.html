<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="do">
<meta property="og:type" content="website">
<meta property="og:title" content="yukai">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="yukai">
<meta property="og:description" content="do">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="wyukai">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>yukai</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">yukai</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/28/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/28/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/" class="post-title-link" itemprop="url">目标检测中的FPN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-28 17:17:24 / 修改时间：18:07:42" itemprop="dateCreated datePublished" datetime="2021-12-28T17:17:24+08:00">2021-12-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/" itemprop="url" rel="index"><span itemprop="name">object detection</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/ssl/" itemprop="url" rel="index"><span itemprop="name">ssl</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/ssl/image-classification/" itemprop="url" rel="index"><span itemprop="name">image classification</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>z 摘录自小纸屑 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148738276">https://zhuanlan.zhihu.com/p/148738276</a></p>
<h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p>FPN结构，特征金字塔网络，主要用作不同尺度的特征融合，从而提高目标检测算法的精度。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228172104469.png" alt="image-20211228172104469"></p>
<p>常见的物体检测算法，其实可以分解为三个递进的阶段：</p>
<ol>
<li><p>Backbone特征提取阶段</p>
<p>Backbone生成的特征，一般按stage划分，分别记作C1、C2、C3、C4、C5、C6、C7等，其中的数字与stage的编号相同，代表的是分辨率减半的次数，如C2代表stage2输出的特征图，分辨率为输入图片的1/4，C5代表，stage5输出的特征图，分辨率为输入图片的1/32。</p>
</li>
<li><p>Neck特征融合阶段</p>
<p>FPN一般将上一步生成的不同分辨率特征作为输入，输出经过融合后的特征。输出的特征一般以P作为编号标记。如FPN的输入是，C2、C3、C4、C5、C6，经过融合后，输出为P2、P3、P4、P5、P6。这个过程可以用数学公式表达：</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228172506926.png" alt="image-20211228172506926"></p>
</li>
<li><p>Head检测阶段</p>
<p>FPN输出融合后的特征后，就可以输入到检测头做具体的物体检测。</p>
</li>
</ol>
<h3 id="2-FPN及其变种结构"><a href="#2-FPN及其变种结构" class="headerlink" title="2. FPN及其变种结构"></a>2. FPN及其变种结构</h3><p>物体检测性能提升，一般主要通过数据增强、改进Backbone、改进FPN、改进检测头、改进loss、改进后处理等6个常用手段。其中FPN自从被提出来，先后迭代了不少版本。大致迭代路径如下图：</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228172706353.png" alt="image-20211228172706353"></p>
<ol>
<li>无融合  </li>
</ol>
<p>无融合，又利用多尺度特征的典型代表SSD，它直接利用不同stage的特征图分别负责不同scale大小物体的检测。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228172812278.png" alt="image-20211228172812278"></p>
<ol>
<li><p>自上而下单向融合</p>
<p>自上而下单向融合的FPN，事实上仍然是当前物体检测模型的主流融合模式。如我们常见的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.01497">Faster RCNN</a>、<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.06870">Mask RCNN</a>、<a href="https://link.zhihu.com/?target=https%3A//pjreddie.com/media/files/papers/YOLOv3.pdf">Yolov3</a>、<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.02002">RetinaNet</a>、<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1712.00726">Cascade RCNN</a>等，具体各个FPN的内部细节如下图。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228172915560.png" alt="image-20211228172915560"></p>
</li>
</ol>
<p><strong>a）Faster/Master/Cascade RCNN中的FPN</strong></p>
<p>Faster/Master/Cascade RCNN中的FPN，利用了C2-C6五个stage的特征，其中C6是从C5直接施加1x1/2的MaxPooling操作得到。FPN融合后得到P2-P6，其中P6直接等于C6，P5是先经过1x1Conv，再经过3x3Conv得到，P2-P4均是先经过1x1Conv，再融合上一层2xUpsample的特征，再经过3x3Conv得到。具体过程可以看上图。</p>
<p><strong>b）RetinaNet中的FPN</strong></p>
<p>RetinaNet中的FPN，利用了C3-C7五个stage的特征，其中C6是从C5直接施加3x3/2的Conv操作得到，C7是从C6直接施加3x3/2的Conv操作得到。FPN融合后得到P3-P7，其中P6、P7直接等于C6、C7，P5是先经过1x1Conv，再经过3x3Conv得到，P3-P4均是先经过1x1Conv，再融合上一层2xUpsample的特征，再经过3x3Conv得到。具体过程可以看上图。</p>
<p>可以看出，RetinaNet基本与Faster/Master/Cascade RCNN中的FPN一脉相承。只是利用的stage的特征略有差别，Faster/Master/Cascade RCNN利用了高分辨率低语义的C2，RetinaNet利用了更低分辨率更高语义的C7。其他都是细微的差别。</p>
<p><strong>c）Yolov3中的FPN</strong></p>
<p>Yolov3中的FPN与上述两个有比较大的区别。首先，Yolov3中的FPN只利用到了C3-C5三个stage的特征；其次，从C5征到P5特征，会先经过5层Conv，然后再经过一层3x3Conv；最后，C3-C4到P3-P4特征，上一层特征会先经过1x1Conv+2xUpsample，然后先与本层特征concatenate，再经过5层Conv，之后经过一层3x3Conv。</p>
<ol>
<li>简单双向融合</li>
</ol>
<p>FPN自从提出来以后，均是只有从上向下的融合，<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.01534">PANet</a>是第一个提出从下向上二次融合的模型，并且PANet就是在Faster/Master/Cascade RCNN中的FPN的基础上，简单增了从下而上的融合路径。看下图。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173101063.png" alt="image-20211228173101063"></p>
<ol>
<li>复杂的双向融合</li>
</ol>
<p>PANet的提出证明了双向融合的有效性，而PANet的双向融合较为简单，因此不少文章在FPN的方向上更进一步，尝试了更复杂的双向融合，如<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1911.09516">ASFF</a>、<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.07392">NAS-FPN</a>和<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1911.09070">BiFPN</a>。</p>
<p><strong>ASFF</strong><br>ASFF（论文：Learning Spatial Fusion for Single-Shot Object Detection）作者在YOLOV3的FPN的基础上，研究了每一个stage再次融合三个stage特征的效果。如下图。其中不同stage特征的融合，采用了注意力机制，这样就可以控制其他stage对本stage特征的贡献度。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173150634.png" alt="image-20211228173150634"></p>
<p><strong>NAS-FPN和BiFPN</strong><br>NAS-FPN和BiFPN，都是google出品，思路也一脉相承，都是在FPN中寻找一个有效的block，然后重复叠加，这样就可以弹性的控制FPN的大小。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173207992.png" alt="image-20211228173207992"></p>
<p>其中BiFPN的具体细节如下图。</p>
<p>改进点1，删除入度为1的节点，如果一个节点只有一个输入边且没有特征融合，那么它将对旨在融合不同特征的特征网络贡献较小。</p>
<p>改进点2，添加跳跃连接。如果原始输入和输出节点处于同一level，则在原始输入和输出节点之间添加一条额外的边，以便在不增加成本的情况下融合更多特征。</p>
<p>改进点3，将BiFPN视作一个基本单元，重复堆叠。不像PANet那样只有一个top-down和bottom-up路径，BiFPN将一对路径视为一个特征层，然后重复多次以得到更多高层特征融合。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173236688.png" alt="image-20211228173236688"></p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173943134.png" alt="image-20211228173943134"></p>
<ol>
<li><p><strong>Recursive-FPN</strong></p>
<p>Paper: <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2006.02334">DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution</a></p>
<p>递归FPN理解起来很容易，就是将传统FPN的融合后的输出，再输入给Backbone，进行二次循环，如下图。</p>
</li>
</ol>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173603992.png" alt="image-20211228173603992"></p>
<p>下图给出了FPN与Recursive-FPN的区别，并且把一个2层的递归FPN展开了，非常简单明了，不做过多介绍。</p>
<p><img src="../images/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84FPN/image-20211228173616868.png" alt="image-20211228173616868"></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148738276">https://zhuanlan.zhihu.com/p/148738276</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/28/ASFF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/28/ASFF/" class="post-title-link" itemprop="url">ASFF</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-28 10:36:52 / 修改时间：17:18:37" itemprop="dateCreated datePublished" datetime="2021-12-28T10:36:52+08:00">2021-12-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/" itemprop="url" rel="index"><span itemprop="name">object detection</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/ssl/" itemprop="url" rel="index"><span itemprop="name">ssl</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/ssl/image-classification/" itemprop="url" rel="index"><span itemprop="name">image classification</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="YOLOV3-ASFF"><a href="#YOLOV3-ASFF" class="headerlink" title="YOLOV3-ASFF"></a>YOLOV3-ASFF</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p>  YOLOV3-ASFF为了解决FPN不同特征尺度之间的不一致问题，提出自适应空间特征融合策略。它通过设置可学习权重因子对不同尺度的特征进行自适应(可学习)融合，通过在空间上过滤冲突信息从而抑制梯度反传时的不一致问题，从而改善了特征的比例不变性，也降低了时间开销。</p>
<p><img src="../images/ASFF/image-20211228151839649.png" alt="image-20211228151839649"></p>
<p>除了自适应空间特征融合，YOLOv3-ASFF在YOLOv3基础上博采众长，集合了MixUp数据增强，学习率cosine衰减策略，异步BN，Guided Anchoring，回归loss改为IoU loss等一系列tricks。 其strong yolov3-608 在COCO2017上达到了 38.8AP + 50fps的效果， 超过了原始YOLOv3-608 ： 33.0AP + 53fps。</p>
<blockquote>
<p>论文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.09516v2.pdf">https://arxiv.org/pdf/1911.09516v2.pdf</a></p>
<p>code: <a target="_blank" rel="noopener" href="https://github.com/GOATmessi7/ASFF">https://github.com/GOATmessi7/ASFF</a></p>
</blockquote>
<h3 id="2-自适应空间特征融合（ASFF）"><a href="#2-自适应空间特征融合（ASFF）" class="headerlink" title="2.自适应空间特征融合（ASFF）"></a>2.自适应空间特征融合（ASFF）</h3><p><img src="../images/ASFF/image-20211228152822280.png" alt="image-20211228152822280"></p>
<p>在上图中， 绿色框里代表一个ASFF模块，X1, X2, X3代表level1, level2, level3三层的特征图，<script type="math/tex">\alpha, \beta, \gamma</script> 则为可学习的权重参数。 加权融合后特征最为Head层的输入。</p>
<p><img src="../images/ASFF/image-20211228153257070.png" alt="image-20211228153257070"></p>
<p>以ASFF-2举例说明：</p>
<ol>
<li>对Level1的特整图（1，512，10，10）压缩通道数， 然后插值后变为（1，256，20，20）的特征向量。</li>
<li>对Level3的特征图进行<script type="math/tex">k=3*3，s=2</script>卷积，输出特征图（1，256，20，20）</li>
<li>Level2层级上特征图不需改变，然后对三个层级的特征图进行<script type="math/tex">k=1*1，s=1</script>的卷积，得到三个层级的空间权重向量（1，16*3， 20，20）</li>
<li>对空间特征降维，并沿通道方向进行softmax操作，得到（1，3，20，20）的权重向量。</li>
<li>特征融合。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ASFF</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, level, rfb=<span class="literal">False</span>, vis=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ASFF, self).__init__()</span><br><span class="line">        self.level = level</span><br><span class="line">        self.dim = [<span class="number">512</span>, <span class="number">256</span>, <span class="number">256</span>]</span><br><span class="line">        self.inter_dim = self.dim[self.level]</span><br><span class="line">        <span class="keyword">if</span> level==<span class="number">0</span>:</span><br><span class="line">            self.stride_level_1 = add_conv(<span class="number">256</span>, self.inter_dim, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">            self.stride_level_2 = add_conv(<span class="number">256</span>, self.inter_dim, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">            self.expand = add_conv(self.inter_dim, <span class="number">1024</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> level==<span class="number">1</span>:</span><br><span class="line">            self.compress_level_0 = add_conv(<span class="number">512</span>, self.inter_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            self.stride_level_2 = add_conv(<span class="number">256</span>, self.inter_dim, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">            self.expand = add_conv(self.inter_dim, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> level==<span class="number">2</span>:</span><br><span class="line">            self.compress_level_0 = add_conv(<span class="number">512</span>, self.inter_dim, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            self.expand = add_conv(self.inter_dim, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        compress_c = <span class="number">8</span> <span class="keyword">if</span> rfb <span class="keyword">else</span> <span class="number">16</span>  <span class="comment">#when adding rfb, we use half number of channels to save memory</span></span><br><span class="line"></span><br><span class="line">        self.weight_level_0 = add_conv(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.weight_level_1 = add_conv(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.weight_level_2 = add_conv(self.inter_dim, compress_c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.weight_levels = nn.Conv2d(compress_c*<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.vis= vis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x_level_0, x_level_1, x_level_2</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.level==<span class="number">0</span>:</span><br><span class="line">            level_0_resized = x_level_0</span><br><span class="line">            level_1_resized = self.stride_level_1(x_level_1)</span><br><span class="line"></span><br><span class="line">            level_2_downsampled_inter =F.max_pool2d(x_level_2, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">            level_2_resized = self.stride_level_2(level_2_downsampled_inter)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.level==<span class="number">1</span>:</span><br><span class="line">            level_0_compressed = self.compress_level_0(x_level_0)    <span class="comment"># [1,512,10,10] --&gt;  [1,256,10,10] </span></span><br><span class="line">            level_0_resized =F.interpolate(level_0_compressed, scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;nearest&#x27;</span>) <span class="comment"># [1,256,20,20]</span></span><br><span class="line">            level_1_resized =x_level_1 <span class="comment"># [1,256,20,20]</span></span><br><span class="line">            level_2_resized =self.stride_level_2(x_level_2) <span class="comment">#[1,256,40,40] --&gt; [1,256, 20,20]</span></span><br><span class="line">        <span class="keyword">elif</span> self.level==<span class="number">2</span>:</span><br><span class="line">            level_0_compressed = self.compress_level_0(x_level_0)</span><br><span class="line">            level_0_resized =F.interpolate(level_0_compressed, scale_factor=<span class="number">4</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">            level_1_resized =F.interpolate(x_level_1, scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">            level_2_resized =x_level_2</span><br><span class="line"></span><br><span class="line">        level_0_weight_v = self.weight_level_0(level_0_resized)  <span class="comment">#[1,16,20,20]</span></span><br><span class="line">        level_1_weight_v = self.weight_level_1(level_1_resized)  <span class="comment">#[1,16,20,20]</span></span><br><span class="line">        level_2_weight_v = self.weight_level_2(level_2_resized)  <span class="comment">#[1,16,20,20]</span></span><br><span class="line">        levels_weight_v = torch.cat((level_0_weight_v, level_1_weight_v, level_2_weight_v),<span class="number">1</span>)  <span class="comment">#[1,48,20,20]</span></span><br><span class="line">        levels_weight = self.weight_levels(levels_weight_v)  <span class="comment"># [1,3,20,20]</span></span><br><span class="line">        </span><br><span class="line">        levels_weight = F.softmax(levels_weight, dim=<span class="number">1</span>)  </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#softmax激活函数对三个通道的各位置进行运算，对不同通道的特征赋予不同的权重（归一化后）,以达到自适应（可学习）的特征融合</span></span><br><span class="line">        fused_out_reduced = level_0_resized * levels_weight[:,<span class="number">0</span>:<span class="number">1</span>,:,:]+\</span><br><span class="line">                            level_1_resized * levels_weight[:,<span class="number">1</span>:<span class="number">2</span>,:,:]+\</span><br><span class="line">                            level_2_resized * levels_weight[:,<span class="number">2</span>:,:,:]</span><br><span class="line"></span><br><span class="line">        out = self.expand(fused_out_reduced)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.vis:</span><br><span class="line">            <span class="keyword">return</span> out, levels_weight, fused_out_reduced.<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = ASFF(level=<span class="number">1</span>)</span><br><span class="line">    l1 = torch.ones(<span class="number">1</span>,<span class="number">512</span>,<span class="number">10</span>,<span class="number">10</span>) <span class="comment">#  FPN --&gt; L1</span></span><br><span class="line">    l2 = torch.ones(<span class="number">1</span>,<span class="number">256</span>,<span class="number">20</span>,<span class="number">20</span>) <span class="comment">#  FPN --&gt; L2</span></span><br><span class="line">    l3 = torch.ones(<span class="number">1</span>,<span class="number">256</span>,<span class="number">40</span>,<span class="number">40</span>) <span class="comment">#  FPN --&gt; L3</span></span><br><span class="line"></span><br><span class="line">    out = model(l1,l2,l3)</span><br><span class="line">    <span class="built_in">print</span>(out.shape) <span class="comment">#[1,256,20,20]</span></span><br></pre></td></tr></table></figure>
<h3 id="3-ASFF的可解释性"><a href="#3-ASFF的可解释性" class="headerlink" title="3. ASFF的可解释性"></a>3. ASFF的可解释性</h3><p>以YOLOv3为例，加入FPN后通过链式法则在backward的时候梯度是这样计算的：</p>
<p><img src="../images/ASFF/image-20211228154404525.png" alt="image-20211228154404525"></p>
<p>通常情况下，特征图尺寸增大通过插值（interpolation）实现，特征图尺寸缩小通过pooling来实现，假设：</p>
<p><img src="../images/ASFF/image-20211228154629987.png" alt="image-20211228154629987"></p>
<p>对于融合运算(add, concat)，相当于对输出特征的activation操作，其导数也将为固定值，可以将它的值同样简化为1。</p>
<p><img src="../images/ASFF/image-20211228154848675.png" alt="image-20211228154848675"><img src="../images/ASFF/image-20211228154856380.png" alt="image-20211228154856380"></p>
<p>最终简化为：</p>
<p><img src="../images/ASFF/image-20211228155128091.png" alt="image-20211228155128091"></p>
<p>若Level1上的某位置上为正样本特征，那如果Level2, Level3上对应位置却为负样本，那么在反向传播的梯度中既包含正样本信息也包含负样本信息，就会造成信息的不一致性，从而降低Level1上各个特征的学习效率。而通过ASFF的方式，反向传播的梯度表达式就变成了：</p>
<p><img src="../images/ASFF/image-20211228155556739.png" alt="image-20211228155556739"></p>
<p>因此可以动态地学习权重参数，进行更优的特征融合。</p>
<p><img src="../images/ASFF/image-20211228155840045.png" alt="image-20211228155840045"></p>
<p>上图可视化的结果进一步阐明了ASFF的有效性。比如大尺度的目标，可以看到斑马实际上是在level1这个特征图上被检测到的，并且观察level1这一层的权重信息<script type="math/tex">\alpha, \beta, \gamma</script>可以发现，对于图中斑马这种大目标更容易被高层的特征捕捉到，因为对于大物体需要更大的感受野和高级语义特征。而对于小目标的检测，可以看到羊更多的是被level2和level3检测到，这也说明了对于小物体更需要底层特征中的细粒度特征来检测。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><a target="_blank" rel="noopener" href="https://qiyuan-z.github.io/2020/04/20/%E8%87%AA%E9%80%82%E5%BA%94%E7%A9%BA%E9%97%B4%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88-(ASFF)/">https://qiyuan-z.github.io/2020/04/20/%E8%87%AA%E9%80%82%E5%BA%94%E7%A9%BA%E9%97%B4%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88-(ASFF)/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wujianming-110117/p/12921308.html">https://www.cnblogs.com/wujianming-110117/p/12921308.html</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/129363523">https://zhuanlan.zhihu.com/p/129363523</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/138816612">https://zhuanlan.zhihu.com/p/138816612</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/21/letterbox/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/21/letterbox/" class="post-title-link" itemprop="url">YOLOv5 自适应图片缩放</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-21 14:32:38" itemprop="dateCreated datePublished" datetime="2021-12-21T14:32:38+08:00">2021-12-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-28 16:06:14" itemprop="dateModified" datetime="2021-12-28T16:06:14+08:00">2021-12-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="自适应图片缩放"><a href="#自适应图片缩放" class="headerlink" title="自适应图片缩放"></a>自适应图片缩放</h3><p>按照以往的经验，目标检测算法在训练和推理阶段都会resize到统一的图像尺寸，YOLOv5在推理阶段采用了自适应的图片缩放trick。</p>
<p><img src="../images/letterbox/format,png.png" alt="img"></p>
<p>在YOLOv5 官方github下有这样一段解释，采用32整数倍的矩形框推理要比resize到等长宽的正方形进行推理的时间减少很多（416 ,416)-&gt;(256 , 416）。</p>
<h3 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h3><p>假设原图尺寸为（523， 699）</p>
<p>（1） 计算长边缩放比例 r = 416 / 699 = 0.5951</p>
<p>（2）将原图等比例缩放 (523，699) —&gt;&gt;  (311, 416)</p>
<p><img src="../images/letterbox/image-20211221174250645.png" alt="image-20211221174250645"></p>
<p>（3） 填充为（416，416），H侧上下需要填充的大小 pad = (416 - 311) / 2 = 52.5</p>
<p><img src="../images/letterbox/new-16400798041612.jpg" alt="new"></p>
<h3 id="推理阶段"><a href="#推理阶段" class="headerlink" title="推理阶段"></a>推理阶段</h3><p>（1） 计算长边缩放比例 r = 416 / 699 = 0.5951</p>
<p>（2）将原图等比例缩放 (523，699) —&gt;&gt;  (311, 416)</p>
<p>（3）原始输入图像缩放后的分辨率（设定为32的倍数）： np.ceil(0.5951 x 523 / 32) x 32,   np.ceil(1 x 699 / 32) x 32 =  (320,416)</p>
<p>（4）计算需要的padding,   宽 padding = (416 - 416) / 2 = 0,     高padding  =  (320 -  311) / 2 = 4.5  (top  4 ,   bottom 5) </p>
<p>（5）填充像素值  （144，144，144）灰色像素</p>
<p>所以推理阶段的分辨率为（320，416）， 在保证图像不失真的情况下，可以显著减少计算量，加快推理速度。</p>
<p><img src="../images/letterbox/new-16400808257133.jpg" alt="new"></p>
<h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterbox</span>(<span class="params">im, new_shape, color=(<span class="params"><span class="number">140</span>,<span class="number">140</span>,<span class="number">140</span></span>), stride=<span class="number">32</span>, auto=<span class="literal">True</span></span>):</span></span><br><span class="line">    shape = im.shape[:<span class="number">2</span>] <span class="comment"># current shape [height, width]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate scale ratio r</span></span><br><span class="line">    r = <span class="built_in">min</span>(new_shape[<span class="number">0</span>] / shape[<span class="number">0</span>], new_shape[<span class="number">1</span>] / shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute padding   new_unpad : [w, h]</span></span><br><span class="line">    new_unpad = <span class="built_in">int</span>(<span class="built_in">round</span>(shape[<span class="number">1</span>] * r)), <span class="built_in">int</span>(<span class="built_in">round</span>(shape[<span class="number">0</span>] * r))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;new_unpad::&#x27;</span>,new_unpad)</span><br><span class="line">    dw = new_shape[<span class="number">1</span>] - new_unpad[<span class="number">0</span>]</span><br><span class="line">    dh = new_shape[<span class="number">0</span>] - new_unpad[<span class="number">1</span>]</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># minimum rectangle</span></span><br><span class="line">    <span class="keyword">if</span> auto:</span><br><span class="line">        dw = np.mod(dw, stride)</span><br><span class="line">        dh = np.mod(dh, stride)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># dw dh for every side</span></span><br><span class="line">    dw /= <span class="number">2</span></span><br><span class="line">    dh /= <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">    <span class="keyword">if</span> shape[::-<span class="number">1</span>] != new_unpad: </span><br><span class="line">        im = cv.resize(im, new_unpad, interpolation=cv.INTER_LINEAR)</span><br><span class="line">    <span class="comment"># padding   if dw &lt; 1： dw = 0</span></span><br><span class="line">    top, bottom = <span class="built_in">int</span>(<span class="built_in">round</span>(dh - <span class="number">0.1</span>)), <span class="built_in">int</span>(<span class="built_in">round</span>(dh + <span class="number">0.1</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;top&#x27;</span>,top, bottom)</span><br><span class="line">    left, right = <span class="built_in">int</span>(<span class="built_in">round</span>(dw - <span class="number">0.1</span>)), <span class="built_in">int</span>(<span class="built_in">round</span>(dw + <span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    im = cv.copyMakeBorder(im, top,bottom,left,right, cv.BORDER_CONSTANT, value=color)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(im.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> im, r, (dw, dh)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    img_path = <span class="string">&quot;D:\\person\\py_code\\list\\R-C.png&quot;</span></span><br><span class="line">    img = cv.imread(img_path)</span><br><span class="line"></span><br><span class="line">    im, r, _ = letterbox(img, (<span class="number">416</span>,<span class="number">416</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(r)</span><br><span class="line">    cv.imwrite(<span class="string">&#x27;D:\\person\\py_code\\list\\new.jpg&#x27;</span>, im)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/10/C-%E7%BA%BF%E7%A8%8B%E9%94%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/10/C-%E7%BA%BF%E7%A8%8B%E9%94%81/" class="post-title-link" itemprop="url">C++线程锁</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-10 14:01:56 / 修改时间：14:17:42" itemprop="dateCreated datePublished" datetime="2021-12-10T14:01:56+08:00">2021-12-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="C-11中的几种线程锁"><a href="#C-11中的几种线程锁" class="headerlink" title="C++11中的几种线程锁"></a>C++11中的几种线程锁</h2><p>线程之间的锁有： 互斥锁，条件锁，自旋锁，读写锁，递归锁。一般情况下，锁的功能是与程序性能成反比。</p>
<ol>
<li><p>互斥锁（Mutex）</p>
<p>互斥锁用于控制多个线程对他们之间共享资源互斥访问的一个信号量，为了避免多个线程在某一时刻同时操作一个共享资源。例如线程池中的有多个空闲线程和一个任务队列。任何是一个线程都要使用互斥锁互斥访问任务队列，以避免多个线程同时访问任务队列以发生错乱。在某一时刻，只有一个线程可以获取互斥锁，在释放互斥锁之前其他线程都不能获取该互斥锁。如果其他线程想要获取这个互斥锁，那么这个线程只能以阻塞方式进行等待。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;list&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mutex&gt;</span></span></span><br><span class="line"></span><br><span class="line">std::list&lt;<span class="keyword">int</span>&gt; some_list;</span><br><span class="line">std::mutex some_mutex;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add_to_list</span><span class="params">(<span class="keyword">int</span> new_value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">std::lock_guard&lt;std::mutex&gt; <span class="title">guard</span><span class="params">(some_mutex)</span></span>;</span><br><span class="line">    some_list.<span class="built_in">push_back</span>(new_value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>条件锁</p>
<p>条件锁就是所谓的条件变量，某一个线程因为某个条件为满足时可以使用条件变量使改程序处于阻塞状态。一旦条件满足以“信号量”的方式唤醒一个因为该条件而被阻塞的线程。最为常见就是在线程池中，起初没有任务时任务队列为空，此时线程池中的线程因为“任务队列为空”这个条件处于阻塞状态。一旦有任务进来，就会以信号量的方式唤醒一个线程来处理这个任务。</p>
</li>
<li><p>自旋锁</p>
<p>假设我们有一个两个处理器core1和core2计算机，现在在这台计算机上运行的程序中有两个线程：T1和T2分别在处理器core1和core2上运行，两个线程之间共享着一个资源。</p>
<p>互斥锁是是一种sleep-waiting的锁。假设线程T1获取互斥锁并且正在core1上运行时，此时线程T2也想要获取互斥锁（pthread_mutex_lock），但是由于T1正在使用互斥锁使得T2被阻塞。当T2处于阻塞状态时，T2被放入到等待队列中去，处理器core2会去处理其他任务而不必一直等待（忙等）。也就是说处理器不会因为线程阻塞而空闲着，它去处理其他事务去了。</p>
<p>而自旋锁就不同了，自旋锁是一种busy-waiting的锁。也就是说，如果T1正在使用自旋锁，而T2也去申请这个自旋锁，此时T2肯定得不到这个自旋锁。与互斥锁相反的是，此时运行T2的处理器core2会一直不断地循环检查锁是否可用（自旋锁请求），直到获取到这个自旋锁为止。</p>
<p>从“自旋锁”的名字也可以看出来，如果一个线程想要获取一个被使用的自旋锁，那么它会一致占用CPU请求这个自旋锁使得CPU不能去做其他的事情，直到获取这个锁为止，这就是“自旋”的含义。</p>
<p>当发生阻塞时，互斥锁可以让CPU去处理其他的任务；而自旋锁让CPU一直不断循环请求获取这个锁。通过两个含义的对比可以我们知道“自旋锁”是比较耗费CPU的。</p>
</li>
<li><p>读写锁 </p>
<p>借助于“读者-写者”问题进行理解， 计算机中某些数据被多个进程共享，对数据库的操作有两种：一种是读操作，就是从数据库中读取数据不会修改数据库中内容；另一种就是写操作，写操作会修改数据库中存放的数据。因此可以得到我们允许在数据库上同时执行多个“读”操作，但是某一时刻只能在数据库上有一个“写”操作来更新数据。这就是一个简单的读者-写者模型。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/07/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/07/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96/" class="post-title-link" itemprop="url">直方图均衡化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-07 18:37:16 / 修改时间：18:39:39" itemprop="dateCreated datePublished" datetime="2021-12-07T18:37:16+08:00">2021-12-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkzNDIxMzE1NQ==&amp;mid=2247486316&amp;idx=1&amp;sn=b55e2b38dc58856e36377a6a37401eb7&amp;chksm=c241e820f53661364efbd707794a7fa4b3b83cac148d73f926e9cf35995079b0d24fdb83d42a&amp;scene=178&amp;cur_album_id=1860258784426672132#rd">阅读原文</a></p>
<p>直方图均衡化，可以对在不同的光线条件下拍摄不同的图片进行均衡化处理，使得这些图片具有大致相同的光照条件。因此，我们可以用在训练模型之前，对图像进行对预处理。</p>
<h2 id="直方图均衡"><a href="#直方图均衡" class="headerlink" title="直方图均衡"></a>直方图均衡</h2><h3 id="1-直方图与对比度"><a href="#1-直方图与对比度" class="headerlink" title="1. 直方图与对比度"></a>1. 直方图与对比度</h3><p>首先，我们看下面的图像：</p>
<p><img src="https://files.mdnice.com/user/6935/fa671e7e-51c5-4886-bae4-0f203dc5968c.png" alt=""></p>
<p>左列为原图，我们在观看的时候，感受很差。为什么很差呢？因为前景(关键区域)与背景太相似，无法很好的得到前景的信息。这就是表明，<strong>这些图像的对比度小，视觉体验很差</strong>。</p>
<p>其中，<strong>对比度是由两个相邻区域的亮度差异产生的</strong>。</p>
<p>对比度是使一个物体与其他物体区别开来的视觉特性上的差异。在视觉感知中，对比度是由物体与其他物体的颜色和亮度差异决定的，而我们的视觉系统对对比度比对绝对亮度更敏感。那么，如何量化一个图像中的对比度呢？我们先了解下直方图。</p>
<p>通过直方图我们可以看到各个灰度级的像素个数，即像素的分布情况。如果图像的大部分的像素都集中在直方图的某个范围，就表示图像中的大部分像素的灰度值差别很小，无法很好地进行分辨图像中的物体。</p>
<p>如原图像的像素值介于$5$~$10$之间（对比度是$10/5=2$）现将其映射到整个区域的输出图像到$0$ ~ $255$（对比度是$255/1=255$），由此可见，对比度得到了很大的提升。</p>
<h3 id="2-直方图的定义"><a href="#2-直方图的定义" class="headerlink" title="2. 直方图的定义"></a>2. 直方图的定义</h3><p>图像的直方图：反应图像强度分布的总体概念。宽泛的来说直方图给出了图像<strong>对比度</strong>、<strong>亮度</strong>和<strong>强度</strong>分布信息。其中，强度就是一幅图像的像素取值，如$[0, 255]$。</p>
<p>其中，公式表示如下：</p>
<script type="math/tex; mode=display">
h\left(r_{k}\right)=n_{k}</script><p>其中，$n_{k}$是图像中灰度级为$r_{k}$的像素个数。 $r_{k}$是第$k$个灰度级，$k=0,1,2…L-1$。由于$r_{k}$的增量是$1$,直方图可以表示为$p(k)=n_{k}$。即，图像中不同灰度级像素出现的次数。</p>
<p><strong>概括来说，直方图就是横坐标表示成像素值，纵坐标表示各个像素值的个数的图。</strong></p>
<p><img src="https://files.mdnice.com/user/6935/d49b14ab-a6c6-4588-a9c5-8fe2d14f7d22.png" alt="直方图举例"></p>
<h3 id="3-直方图均衡化的引入"><a href="#3-直方图均衡化的引入" class="headerlink" title="3.直方图均衡化的引入"></a>3.直方图均衡化的引入</h3><p>若一幅图像的像素倾向于占据整个可能的灰度级并且分布均匀，则该图像有较高的对比度并且图像展示效果会相对好，于是便引出图像直方图均衡化，对图像会有很强的增强效果。</p>
<h4 id="3-1-直方图归一化"><a href="#3-1-直方图归一化" class="headerlink" title="3.1 直方图归一化"></a>3.1 直方图归一化</h4><p>先了解<strong>直方图归一化</strong>的概念，公式为：<br>$p(r_{k})=n_{k}/n$</p>
<ul>
<li>$n$ 是图像的像素总数（如一幅$32*32$的图像，像素总数就是$1024$）。</li>
<li>$n_{k}$是图像中灰度级为$r_{k}$的像素个数</li>
<li>$r_{k}$是第$k$个灰度级，$k = 0,1,2,…,L-1$</li>
</ul>
<p>因此，该函数主要有以下几个特性：  </p>
<ol>
<li>使函数值压缩到[0,1]区间。</li>
<li>给出灰度级$r_{k}$在图像中出现的概率密度统计。</li>
</ol>
<h4 id="3-2-直方图均衡化"><a href="#3-2-直方图均衡化" class="headerlink" title="3.2 直方图均衡化"></a>3.2 直方图均衡化</h4><p>直方图均衡化是建立在直方图归一化的基础之上。直方图均衡化的公式如下所示：</p>
<script type="math/tex; mode=display">
s_{k}=\sum_{j=0}^{k}\frac{n_{j}}{n}, k=0, 1, 2,,,L-1</script><p><strong>注：</strong></p>
<ul>
<li>$n$是图像中像素的总和</li>
<li>$n_{j}$是当前灰度级的像素个数</li>
<li>$L$是图像中可能的灰度级总数</li>
</ul>
<p>其中，直方图均衡化是采用的灰度级变换：$s = T(r)$，目的是<strong>欲将原始图的直方图变换为均匀分布的形式，这样就增加了像素灰度值的动态范围，从而达到增强图像整体对比度的效果</strong>。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 读取图像并转换为灰度图</span></span><br><span class="line">    img = cv2.imread(<span class="string">r&#x27;./imgs/boy.png&#x27;</span>)</span><br><span class="line">    <span class="comment"># 图像的灰度级范围是0~255</span></span><br><span class="line">    <span class="comment"># grayHist = cv2.calcHist([img], [0], None, [256], [0, 256])</span></span><br><span class="line">    (b, g, r) = cv2.split(img)</span><br><span class="line">    bH = cv2.equalizeHist(b)</span><br><span class="line">    gH = cv2.equalizeHist(g)</span><br><span class="line">    rH = cv2.equalizeHist(r)</span><br><span class="line">    <span class="comment"># 合并每一个通道</span></span><br><span class="line">    result = cv2.merge((bH, gH, rH))</span><br><span class="line">    res = np.hstack((img, result))</span><br><span class="line">    cv2.imshow(<span class="string">&quot;dst&quot;</span>, res)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>不能使用库函数，需要写出详细的直方图均衡化的过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hist_equalization</span>(<span class="params">intput_signal</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    直方图均衡（适用于灰度图）</span></span><br><span class="line"><span class="string">    :param intput_signal:   输入图像</span></span><br><span class="line"><span class="string">    :return:    直方图均衡化后的输出图像</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    output_signal = np.copy(intput_signal)   <span class="comment"># 输出图像，初始化为输入</span></span><br><span class="line"></span><br><span class="line">    intput_signal_cp = np.copy(intput_signal) <span class="comment"># 输入图像的副本</span></span><br><span class="line"></span><br><span class="line">    m, n = intput_signal_cp.shape <span class="comment"># 输入图像的尺寸（行、列）</span></span><br><span class="line"></span><br><span class="line">    pixel_total_num = m * n  <span class="comment"># 输入图像的像素点总数</span></span><br><span class="line"></span><br><span class="line">    p_r = []   <span class="comment"># 输入图像的概率密度</span></span><br><span class="line">    p_s = []   <span class="comment"># 输出图像的概率密度</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求输入图像的概率密度函数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">        p_r.append(np.<span class="built_in">sum</span>(intput_signal_cp == i) / pixel_total_num)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求输出图像的概率密度函数</span></span><br><span class="line">    single_pixel_class_probobility_t = <span class="number">0</span>  <span class="comment"># 临时存储某一灰度级的概率</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">        single_pixel_class_probobility_t = single_pixel_class_probobility_t + p_r[i]</span><br><span class="line">        p_s.append(single_pixel_class_probobility_t)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求解变换后的输出图像</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">256</span>):</span><br><span class="line">        output_signal[np.where(intput_signal_cp == i)] = <span class="number">255</span> * p_s[i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_signal</span><br></pre></td></tr></table></figure>
<p><img src="https://files.mdnice.com/user/6935/292f5124-29b1-405f-ba06-19b25dbff6d2.jpg" alt="直方图均衡化举例"></p>
<p>其中关于像素的统计量如下：</p>
<p><img src="https://files.mdnice.com/user/6935/f9864061-b117-4891-b67e-aae1eb2bed01.png" alt="直方图统计对比"></p>
<h3 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h3><p>目前，基本的图像直方图均衡已经说完了，但是如果我们仔细看上图，会发现均衡化处理后对比度大大增强了，但是这个<strong>boy</strong>好像有点太亮了，这是因为这个直方图均衡化操作是对全局进行均衡化，直方图覆盖的范围太大，反而会丢失<strong>boy</strong>的一些信息。</p>
<p>因此，明天我们会继续沿着直方图均衡引入<strong>自适应直方图均衡化(AHE)</strong> 以及 <strong>限制对比度自适应直方图均衡化(CLAHE)</strong> 等直方图均衡化算法。</p>
<h3 id="5-参考"><a href="#5-参考" class="headerlink" title="5. 参考"></a>5. 参考</h3><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/IT_charge/article/details/105560087">https://blog.csdn.net/IT_charge/article/details/105560087</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/98541241">https://zhuanlan.zhihu.com/p/98541241</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/rocling/article/details/104559472">https://blog.csdn.net/rocling/article/details/104559472</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/07/GANomaly/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/07/GANomaly/" class="post-title-link" itemprop="url">GANomaly</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-07 16:45:28 / 修改时间：17:42:37" itemprop="dateCreated datePublished" datetime="2021-12-07T16:45:28+08:00">2021-12-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="GANomaly"><a href="#GANomaly" class="headerlink" title="GANomaly"></a>GANomaly</h2><h3 id="一，简介"><a href="#一，简介" class="headerlink" title="一，简介"></a>一，简介</h3><p> 异常检测旨在只使用正常样本建模从而能够区分OK样本与NG样本，所面对的场景的数据分布极不平衡， 通常OK样本非常多，而NG样本非常少。</p>
<p>自编码器是异常检测算法中比较经典的模型，它利用大量OK训练一个自编码网络，然后通过原图与重构图像之间的重构误差来检测NG样本，但该方法非常容易受噪声影响，其对NG样本也能够重建，导致所谓的重构误差“崩塌”。</p>
<p><img src="../images/GANomaly/image-20211207165552708.png" alt="image-20211207165552708"></p>
<p>GANomaly 采用编码器-解码器-编码器的模型结构， 同时对“原图-》重建图” 及“原图的高维特征编码-&gt;重建图的高维特征编码”进行重构误差约束。另外引入生成对抗网络的对抗训练思想， Encoder-Decoder-Encoder结构当作生成网络G-Net， 又定义了一个判别网络D-Net。</p>
<p>推理接断，用于推断异常的不是原图和重建图的差异，而是第一部分编码器产生的隐空间特征（原图的编码）和第二部分编码器产生的隐空间特征（重建图的编码）的差异。这种方法更关注图片实质内容的差异，对图片中的微小变化不敏感，因而能解决自编码器中易受噪声影响的问题，<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=鲁棒性&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;article&quot;%2C&quot;sourceId&quot;%3A47832951}">鲁棒性</a>更好。</p>
<h3 id="二，-网络结构"><a href="#二，-网络结构" class="headerlink" title="二， 网络结构"></a>二， 网络结构</h3><p><img src="../images/GANomaly/image-20211207172343665.png" alt="image-20211207172343665"></p>
<p>（1） G-Net， Encoder 1- Decoder - Encoder 2 ,   自编码器结构参考了DCGAN， Encoder 1E将一张3通道的图片映射为一个n维的向量，Decoder则为Encoder的逆过程。Encoder2将重建出的图像在编码为一个n维的向量。</p>
<p>（2）D_Net， 判别网络用于区分原图和重建图，即要将原图判别为真，将重建图判别为假。它的结构和第一个子网络的解码网络是一样的。D-Net的引入，是为了引入对抗训练思想，旨在学到更好的G-Net。</p>
<h3 id="三，损失函数"><a href="#三，损失函数" class="headerlink" title="三，损失函数"></a>三，损失函数</h3><p>本文包含三个子网络，每个子网络对应一个损失函数。</p>
<p><strong>第一个子网络</strong>的损失是自编码器的重建损失，这里借鉴了<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.07004">pix2pix</a>文章中生成网络的损失，采用的是L1损失，而不是L2损失。因为采用L2损失生成的图像通常比采用L1生成的图像要模糊。</p>
<p><img src="../images/GANomaly/image-20211207173018540.png" alt="image-20211207173018540"></p>
<p><strong>第二个子网络</strong>的损失是编码网络的损失，这里需要比对的是原图和重建图在高一层抽象空间中的差异，即两个bottleneck(上文中的bottleneck1和bottleneck2)间的差异，采用的是L2损失。</p>
<p><img src="../images/GANomaly/image-20211207173058603.png" alt="image-20211207173058603"></p>
<p><strong>第三个子网络</strong>的损失是常规的GAN中判别网络的损失，这里采用的是二分类的交叉熵损失，论文中采用的L2。</p>
<p><img src="../images/GANomaly/image-20211207173143827.png" alt="image-20211207173143827"></p>
<h3 id="四，-训练及推理"><a href="#四，-训练及推理" class="headerlink" title="四， 训练及推理"></a>四， 训练及推理</h3><p>本文采用的训练策略和常规的GAN一样的，即交替地优化D-Net和G-Net。</p>
<p>（1） <strong>优化D-Net</strong>时，采用的损失为上述第三个子网络的损失， 输入为 concat(input_real, input_fake)。input_fake由G-Net生成，在训练D-Net时，G-Net参数固定。</p>
<p><img src="../images/GANomaly/image-20211207173603644.png" alt="image-20211207173603644"></p>
<p>（2） <strong>优化G-Net</strong>时，采用的损失比较复杂：</p>
<p><img src="../images/GANomaly/image-20211207173744620.png" alt="image-20211207173744620"></p>
<p>主体为重建损失Lrec，编码损失Lenc为重建损失的一个约束，对抗损失Ladv则用来和D-Net博弈。需要注意的一点是，这里的对抗损失的输入对象和优化D-Net时的输入对象是不一样的，这里的 input_d为input_fake，这和常规GAN的训练是一致的。</p>
<h3 id="五，推理"><a href="#五，推理" class="headerlink" title="五，推理"></a>五，推理</h3><p>前面提到，本文采用的推断方式和一般的基于自编码器的异常检测方法是不一样的。这里推断以来的不是重建损失Lrec，而是编码损失Lenc。具体而言，网络训练收敛以后，我们可以计算得到所有OK样本中的Lenc值，选取其中最大的作为判别阈值。推断时，给定一张图片，我们可以利用学好的网络，计算其 Lenc值，如果它小于判别阈值则判断为OK样本(正常样本)，大于则判断为NG样本(异常样本)。 </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/27/Frameworks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/27/Frameworks/" class="post-title-link" itemprop="url">Caffe、TensorFlow及Pytorch通道维度顺序</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-27 10:34:05" itemprop="dateCreated datePublished" datetime="2021-10-27T10:34:05+08:00">2021-10-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-08 10:23:49" itemprop="dateModified" datetime="2021-11-08T10:23:49+08:00">2021-11-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DL-Framework/" itemprop="url" rel="index"><span itemprop="name">DL Framework</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DL-Framework/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/DL-Framework/deep-learning/object-detection/" itemprop="url" rel="index"><span itemprop="name">object detection.</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li>Caffe：NCHW</li>
<li>TensorFlow：默认为NHWC（但可以设置为NCHW）, NHWC的访存局部性更好（cache利用率高），NCHW需要等所有通道输入准备好才能得到最终输出结果，需要占用较大的临时空间。</li>
<li><p>Pytorch：NCHW</p>
</li>
<li><p>NCHW为NVIDIA Cudnn默认格式， 使用GPU加速时用NCHW格式速度大部分情况下会更快。</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/25/AdaFocus/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/25/AdaFocus/" class="post-title-link" itemprop="url">AdaFocus算法记录</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-10-25 14:13:33 / 修改时间：15:46:27" itemprop="dateCreated datePublished" datetime="2021-10-25T14:13:33+08:00">2021-10-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/" itemprop="url" rel="index"><span itemprop="name">object detection.</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>转载自：计算机视觉Daily，个人学习记录</p>
<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>AdaFocus为被ICCV-2021会议录用为Oral Presentation的一篇文章：Adaptive Focus for Efficient Video Recognition。 其从空间特征角度出发，从降低空间冗余性来实现高效视频识别。</p>
<p>现有高效视频识别算法往往关注于降低视频的时间冗余性（即将计算集中于视频的部分关键帧），如图1 (b)。本文则发现，降低视频的空间冗余性（即寻找和重点处理视频帧中最关键的图像区域），如图1 (c)，同样是一种效果显著、值得探索的方法；且后者与前者有效互补（即完全可以同时建模时空冗余性，例如关注于关键帧中的关键区域），如图1 (d)。在方法上，本文提出了一个通用于大多数网络的AdaFocus框架，在同等精度的条件下，相较AR-Net (ECCV-2020)将计算开销降低了2.1-3.2倍，将TSM的GPU实测推理速度加快了1.4倍。</p>
<p><img src="../images/AdaFocus/image-20211025143754892.png" alt="image-20211025143754892"></p>
<ul>
<li>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.03245.pdf">https://arxiv.org/pdf/2105.03245.pdf</a></li>
<li>Code：<a target="_blank" rel="noopener" href="https://github.com/blackfeather-wang/AdaFocus">https://github.com/blackfeather-wang/AdaFocus</a></li>
<li>B站介绍：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1vb4y1a7sD/">https://www.bilibili.com/video/BV1vb4y1a7sD/</a></li>
<li>作者个人网站：<a target="_blank" rel="noopener" href="https://www.rainforest-wang.cool/">https://www.rainforest-wang.cool/</a></li>
</ul>
<h2 id="2-研究动机"><a href="#2-研究动机" class="headerlink" title="2. 研究动机"></a>2. 研究动机</h2><p>相较于图像，视频识别是一个分布范围更广、应用场景更多的任务。如下图所示，每分钟，即有超过300小时的视频上传至YouTube；至2022年，超过82%的消费互联网流量将由在线视频组成。自动识别这些海量视频中的人类行为、事件、紧急情况等内容，对于视频推荐、监控等受众广泛的实际应用具有重要意义。</p>
<p><img src="../images/AdaFocus/image-20211025143955391.png" alt="image-20211025143955391"></p>
<p>近年来，已有很多基于深度学习的视频识别算法取得了较佳的性能，如TSM、SlowFast、I3D等。然而，一个严重的问题是，相较于图像，使用深度神经网络处理视频通常会引入很大的计算开销。如下图所示，将ResNet-50应用于视频识别将使运算量（FLOPs）扩大8-75倍。</p>
<p><img src="../images/AdaFocus/image-20211025144020950.png" alt="image-20211025144020950"></p>
<p>因此，一个关键问题在于，如何降低视频识别模型的计算开销。一个非常自然的想法是从视频的时间维度入手：一方面，相邻的视频帧之间往往具有较大的相似性，逐帧处理将引入冗余计算；另一方面，并非全部视频帧的内容都与识别任务相关。现有工作大多从这一时间冗余性出发，动态寻找视频中的若干关键帧进行重点处理，以降低计算成本，如下图所示。</p>
<p><img src="../images/AdaFocus/image-20211025144125135.png" alt="image-20211025144125135"></p>
<p>但是，值得注意的一点是，我们发现，目前尚未有工作关注于视频中的空间冗余性。具体而言，在每一帧视频中，事实上只有一部分空间区域与识别任务相关，例如下图中的运动员、起跳动作、水花等。</p>
<p><img src="../images/AdaFocus/image-20211025144147038.png" alt="image-20211025144147038"></p>
<p>出于这一点，本文以回答图6中的两个问题作为主线：</p>
<ul>
<li><ol>
<li><strong>空间冗余性是否可以用于实现高效视频识别？</strong>假如我们能找到每一视频帧中的关键区域，并将主要的计算集中于这些更有价值的部分，而尽可能略过其他任务相关信息较少的区域，理论上，我们就可以显著降低网络的计算开销（事实上，我们之前基于单张图像验证过类似做法的效果：NeurIPS 2020 | Glance and Focus: 通用、高效的神经网络自适应推理框架：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/266306870）。">https://zhuanlan.zhihu.com/p/266306870）。</a></li>
<li><strong>空间、时间冗余性是否互补？</strong>若上述假设成立的话，它应当可与现存的、基于时间冗余性的工作相结合，因为我们完全可以先找到少数关键帧，再仅在这些帧中寻找关键的图像区域进行重点处理。</li>
</ol>
</li>
</ul>
<p><img src="../images/AdaFocus/image-20211025144336382.png" alt="image-20211025144336382"></p>
<h2 id="3-核心思想"><a href="#3-核心思想" class="headerlink" title="3. 核心思想"></a>3. 核心思想</h2><p>首先为了回答问题1，作者设计了一个AdaFocus框架，其结构如下图所示。</p>
<p><img src="../images/AdaFocus/image-20211025144442968.png" alt="image-20211025144442968"></p>
<p>此处我们假设视频帧按时间次序逐个输入网络，AdaFocus使用四个组件对其进行处理。</p>
<ol>
<li>全局CNN   <script type="math/tex">f(G)</script>  （Global CNN）是一个轻量化的卷积网络（例如MobileNet-V2)，用于以低成本对每一帧视频进行粗略处理，获取其空间分布信息。</li>
<li>策略网络 <script type="math/tex">\pi</script>（Policy Network）是一个循环神经网络（RNN），以<script type="math/tex">f(g)</script>的提取出的特征图作为输入，用于整合到目前为止所有视频帧的信息，进而决定当前帧中包含最重要信息的一个图像小块（patch）的位置。值得注意的是由于取得patch的crop操作不可求导，<script type="math/tex">\pi</script>是使用强化学习中的策略梯度方法（policy gradient)训练的。</li>
<li>局部CNN<script type="math/tex">f(L)</script>（Local CNN）是一个容量大、准确率高但参数量和计算开销较大的卷积网络（例如ResNet），仅处理策略网络<script type="math/tex">\pi</script> 选择出的局部patch，由于patch的空间尺寸小于原图，处理其的计算开销显著低于处理整个视频帧。</li>
<li>分类器<script type="math/tex">f(c)</script>（Classifier）为另一个循环神经网络（RNN），输入为<script type="math/tex">f(G)</script>和 <script type="math/tex">f(L)</script>输出特征的并联，用于整合过去所有视频帧的信息，以得到目前最优的识别结果<script type="math/tex">p_t</script>（t表示帧序号)。</li>
</ol>
<h2 id="4-主要贡献点"><a href="#4-主要贡献点" class="headerlink" title="4. 主要贡献点"></a>4. 主要贡献点</h2><p>（1）在现有的基于时间冗余性的方法之外，思考利用空间冗余性实现高效视频识别；</p>
<p>（2）基于强化学习，提出了一种在理论上和实测速度上效果都比较明显的通用框架，AdaFocus；</p>
<p>（3）在五个数据集上进行了实验，包括与其他通用框架的比较和部署于现有高效识别网络（例如TSM）上的效果等。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/22/%E6%97%A5%E5%B8%B8%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/22/%E6%97%A5%E5%B8%B8%E8%AE%B0%E5%BD%95/" class="post-title-link" itemprop="url">日常记录</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-22 14:36:24" itemprop="dateCreated datePublished" datetime="2021-10-22T14:36:24+08:00">2021-10-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-19 15:15:08" itemprop="dateModified" datetime="2021-11-19T15:15:08+08:00">2021-11-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li><p>Papers with Codes 是一个总结了机器学习论文及其代码实现的网站，这个网站最好的地方就是对机器学习做了任务分类，检索对应的模型非常方便。</p>
<ul>
<li>地址：<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https%3A%2F%2Fpaperswithcode.com">https://paperswithcode.com</a></li>
</ul>
</li>
<li><p>ROC/AUC、真正率（TPR）、假正率（FPR）  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46714763">https://zhuanlan.zhihu.com/p/46714763</a></p>
</li>
<li><p>缺陷检测数据集  <a target="_blank" rel="noopener" href="https://blog.csdn.net/gonggongjie/article/details/112544136">https://blog.csdn.net/gonggongjie/article/details/112544136</a></p>
</li>
<li><p>Anomaly Detection on MVTec AD   <a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad">https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad</a></p>
<p><img src="../images/%E6%97%A5%E5%B8%B8%E8%AE%B0%E5%BD%95/image-20211119151444496.png" alt="image-20211119151444496"></p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/20/FixMatch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="wyukai">
      <meta itemprop="description" content="do">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yukai">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/20/FixMatch/" class="post-title-link" itemprop="url">FixMatch算法记录</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-20 18:00:00" itemprop="dateCreated datePublished" datetime="2021-10-20T18:00:00+08:00">2021-10-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-25 15:47:25" itemprop="dateModified" datetime="2021-10-25T15:47:25+08:00">2021-10-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/object-detection/" itemprop="url" rel="index"><span itemprop="name">object detection.</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h2><p>&emsp;&emsp;深度学习在具体的商业落地场景中需要依赖于海量的数据。算法，算力，数据是驱动Deep Learning 运行的三大动力， 而数据又是其中重要影响因素，模型效果80%靠数据，20%靠算法。在工业场景中，面对众多非标准化产品，频繁更换型号的场景，进行数据标准的成本是巨大的，而且客户对某一款产品的算法落地时间有限制， 且如果模型对相似型号的兼容性较差，也会引起客户对AI方案的不信任。</p>
<hr>
<p>&emsp;&emsp;在此种背景下，如何利用大量未标注的图像以及部分已标注的图像来提高模型的性能就变得尤为重要。其中，半监督学习（SSL）就是一种值得尝试的方案，Fix-Match, 是谷歌Google Brain 提出的一种半监督学习方法，对于解决数据收集困难，标注成本高的CV问题会有一定的帮助。</p>
<ul>
<li>FixMatch： Simplifying Semi-Supervised Learning with Consistency and Confidence</li>
<li>主要贡献： 利用一致性正则化（ Consistency regularization）和伪标签（pseudo-labeling）技术进行无监督训练。SOTA 精度，其中CIFAR-10有250个标注，准确率为94.93%。甚至仅使用10张带有标注的图在CIFAR-10上达到78％精度。</li>
<li>论文： <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.07685">https://arxiv.org/abs/2001.07685</a></li>
<li>code： <a target="_blank" rel="noopener" href="https://github.com/google-research/fixmatch">https://github.com/google-research/fixmatch</a></li>
</ul>
<h2 id="2-半监督学习"><a href="#2-半监督学习" class="headerlink" title="2. 半监督学习"></a>2. 半监督学习</h2><p>​        半监督学习（Semi-supervised learning）是一种学习方法，其使用少量标记的数据和大量未标记的数据进行学习。相对于监督学习（Supervised Learning）而言，最大的优势是无需为所有数据准备标签。</p>
<p>​        FixMatch使用的数据集也为带所有标注的数据集，如CIFAR-10， 因为算法训练的要求，需要姜训练数据中的一部分标签删掉， 换句话说，其训练数据为一部分有标签数据 + 一部分无标签数据。</p>
<h2 id="3-核心思想"><a href="#3-核心思想" class="headerlink" title="3. 核心思想"></a>3. 核心思想</h2><p><img src="/images/FixMatch/v2-21388836987e72bff390f7f4a3ade136_r.jpg" alt="v2-21388836987e72bff390f7f4a3ade136_r"></p>
<ol>
<li>整体分为两个部分， 有监督训练部分和无监督训练部分。</li>
</ol>
<ul>
<li>监督训练部分， 利用labeled数据进行监督训练，得到Model A</li>
<li>无监督训练部分， 首先由原图生成<strong>弱增强</strong>数据，通过Model A获得<strong>伪标签</strong>（pseudo-label）, 然后使用<strong>交叉熵损失</strong>利用该伪标签去监督<strong>强增强</strong>的输出值， 特别地，FixMatch仅使用具有<strong>高置信度</strong>的未标记样本参与无监督训练部分。</li>
</ul>
<ol>
<li>无监督训练部分包含两种策略，<strong>一致性正则化及伪标签训练</strong>。</li>
</ol>
<ul>
<li>一致性正则化是当前半监督SOTA工作中一个重要的组件，其建立在一个基本假设：相同图片经过不同扰动（增强）经过网络会输出相同预测结果，FixMatch是对弱增强图像与强增强图像之间的进行一致性正则化，但是其没有使用两种图像的概率分布一致，而是使用弱增强的数据制作了伪标签，这样就自然需要使用交叉熵进行一致性正则化了。</li>
<li>伪标签是利用模型本身为未标记数据获取人工标签的思想。通常是使用“hard”标签，也就是argmax获取的onehot标签，仅保留最大类概率超过阈值的标签。</li>
</ul>
<p><strong>3. Why Work ?</strong></p>
<p>​        无监督训练过程实际上是一个孪生网络，可以提取到图片的有用特征。弱增强不至于图像失真，再加上输出伪标签阈值的设置，极大程度上降低了引入错误标签噪声的可能性。而仅仅使用弱增强可能会导致训练过拟合，无法提取到本质的特征，所以使用强增强。强增强带来图片的严重失真，但是依然是保留足够可以辨认类别的特征。有监督和无监督混合训练，逐步提高模型的表达能力。</p>
<h2 id="4-细节"><a href="#4-细节" class="headerlink" title="4. 细节"></a>4. 细节</h2><ol>
<li>数据增强方式<ul>
<li>弱增强：用标准的翻转和平移策略， 50%的概率进行flip和12.5%的概率进行shift，包括水平和竖直方向。</li>
<li>强增强：输出严重失真的输入图像，先使用RandAugment 或 CTAugment，再使用 CutOut 增强</li>
</ul>
</li>
<li>网络模型<ul>
<li>FixMatch使用 Wide-Resnet 变体作为基础体系结构，记为 Wide-Resnet-28-2，其深度为 28，扩展因子为 2。因此，此模型的宽度是 ResNet 的两倍。</li>
</ul>
</li>
<li>算法流程</li>
</ol>
<p><img src="/images/FixMatch/image-20211022153823958.png" alt="image-20211022153823958"></p>
<p>（1）Input：准备了batch=B的有标签数据和batch=μB 的无标签数据，其中μ是无标签数据的比例；</p>
<p>（2）监督训练：对于在标注数据的监督训练，将常规的交叉熵损失 H()用于分类任务。有标签数据的损失记为ls，如伪代码中第2行所示；</p>
<p>（3）生成伪标签：对无标签数据分别应用弱增强和强增强得到增强后的图形，再送给模型得到预测值，并将弱增强对应的预测值通过 argmax 获得伪标签；</p>
<p>（4）一致性正则化：将强增强对应的预测值与弱增强对应的伪标签进行交叉熵损失 H()计算，未标注数据的损失由 lu 表示，如伪代码中的第7行所示；式τ表示伪标签的阈值；</p>
<p>（5）完整损失函数：最后，我们将ls和lu损失相结合，如伪代码第8行所示，对其进行优化以改进模型，其中，λu 是未标记数据对应损失的权重。</p>
<h2 id="5-实验结果"><a href="#5-实验结果" class="headerlink" title="5. 实验结果"></a>5. 实验结果</h2><p>作者分别在CIFAR和SVHM等数据集上进行了训练测试，模型表现超过之前的网络。具体如下：</p>
<p><img src="/images/FixMatch/image-20211022154528429.png" alt="image-20211022154528429"></p>
<ol>
<li>对于极端缺少标注的场景，仅仅使用每个类别1张共10张标注的图片就可以达到78%的最大accuracy，当然这种做法和挑选的样本质量有关，作者也做了相关实验论证。不过也证明本文的方法的确work。</li>
</ol>
<p><img src="/images/FixMatch/image-20211022154712637.png" alt="image-20211022154712637"></p>
<ol>
<li><p>另外还有一些具体的调参实验，总的来说，通过FixMatch，我们可以得到以下结论：</p>
<p>（1）使用具有高置信度的未标记数据参与训练效果比较好(Argmax)；</p>
<p>（2）适当增加batch中未标记数据的比例有助于提高识别精度；</p>
<p>（3）T越小（即分布越尖锐），则精度会越高(Sharppen Method)。</p>
<p>总的来说，半监督学习是一种好方法，因为其是一种可以在开始高成本之前使用的方法。</p>
</li>
</ol>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li>FixMAtch是半监督领域的一篇经典论文，其做法简单有效，使用图像增强技术进行伪标签学习和一致性正则化训练，在CIFAR等多个数据集上仅仅利用少量的标注图片就可以达到一个不错的效果，这对于获取标注困难的场景非常有意义。例如在工业应用领域，可能会有海量数据，但是现实限制可能无法都进行人工标注，因此可以尝试利用半监督训练的方法，非常值得借鉴。</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/165337501">https://zhuanlan.zhihu.com/p/165337501</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/422930830">https://zhuanlan.zhihu.com/p/422930830</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/340474319">https://zhuanlan.zhihu.com/p/340474319</a></li>
</ul>
<p>个人记录学习</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">wyukai</p>
  <div class="site-description" itemprop="description">do</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wyukai</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
